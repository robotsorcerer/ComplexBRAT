\section{Decomposition Scheme.}
\label{sec:methods}
 %:
\begin{comment}\subsection{Decomposition Rationale.}
%
%By definition, backward reachable tubes (BRTs) are the sublevel sets of the viscosity solutions to 
The numerical resolution of hyperbolic\footnote{Hyperbolic PDEs are those that depend on at most the first-order partial derivatives of the vector field under consideration.} HJ PDEs~\cite{Mitchell2020} in a viscosity sense comes from the resolution of scalar continuous conservation laws e.g. the conservation of electric charge density, $\rho$ (in $\text{Coulombs}/m^{2}$), of an electromagnetic field  i.e.,
%
\begin{align}
\dfrac{\partial \rho}{\partial t} +(\nabla \cdot \bm{J}) = 0
\label{eq:conserve}
\end{align}
%
where $\bm{J}$ is the current density (in $Amperes/m^{2}$). If we relax the viscous properties of these conservation laws, they are effectively reduced to compressible inviscid  Euler equations with discontinuities~\cite{LevelSetsBook}. The discontinuities in the Euler dynamics of such inviscid conservation equations make their  resolution extendable to the numerical solution of HJI PDEs. We restate the following culminations from \cite{OsherShuENO} to build the case for our decomposition scheme.
%
\begin{remark}
The solution (\eg $\rho$ from \eqref{eq:conserve}) to a conservation law is tantamount to the derivative of the solution to an HJ PDE \textbf{along a single spatial dimension}.
\end{remark}
%
%
\begin{remark}
The solution of an HJ PDE, e.g. $\bm{u}$ or $\bm{v}$ in \eqref{eq:lower_visc}, \textbf{along a single spatial dimension}, is the integral of the solution of a scalar conservative equation, \eg $\rho$ in \eqref{eq:conserve}.
\label{rem:integral_conserve}
\end{remark}
%
%The above two remarks are conclusions drawn from \cite{OsherShuENO}. 
Remark \autoref{rem:integral_conserve} informs us that HJ solutions obtained by integrating conservation law PDEs that possess discontinuous dynamics may lead to kinks in the HJ solutions\footnote{Since the integral of a discontinuity is a kink.}. For our proposed decomposition of BRS/BRT, if a  decomposed HJ PDE's solution (to be introduced shortly) develops kinks in its solution, a form of bang-bang control needs be considered if indeed the partial derivatives are discontinuous in those regions of the state space. If the conservation law does not develop a delta function, we can discount accounting for these discontinuities as the HJ solution will be continuous.  In the level set methods, to assure a monotone decrement in the value function, Lax-Friedrich schemes with carefully chosen artificial dissipation coefficients are used in computing solutions to the discretized Hamiltonian in order to account for constant or varying spatial dynamics\footnote{Although in the level sets toolbox, dissipation coefficients are chosen with the assumption that the underlying dynamics are spatially constant; this is very limiting for environments with unpredictable dynamics.} and alleviate possible numerical inconsistencies in computed gradients to the HJ PDE\footnote{This is discussed at length in ~\cite[\S5.3.1]{LevelSetsBook}.}.
\end{comment} 


Multilinear compositions of linear forms are an efficient way of manipulating complex systems.   Higher-order tensors, in particular, are increasingly playing crucial roles in the storage, analysis, and use of high-dimensional data. Applications range from deep learning, higher-order statistics, chemometrics, psychometrics to signal processing inter alia. Evidence abounds that linearized nonlinear system dynamics, truncated at a reduced order $r$-th (e.g. in linearized power series expansions\cite{iDG, JacobsonMayne, Mitter1966, McReynolds1967}) admit higher precision and accuracy of the approximation of the underlying nonlinear system since the moments and accumulations of higher-order dynamics are equivalent to the power series expansion coefficients.  In what follows, we introduce a multilinear decomposition scheme aimed at decomposition of large backward reachable tubes in order to alleviate the exponential complexity of mesh constraints; it is an iterative scheme that generates separable reduced order models (ROM) of the original value function, which are respectively compactly representable on a mesh -- making our method amenable to resolving terminal value functions using level sets methods. 


\subsection{ROM Construction}
 \noindent %Innovative incrementally resolved value function approaches, rather than brute-force global analyses(\eg~\cite{Bansal, Ames2014}) are the obvious alternative for the \textit{practical} resolution of the optimal control problem of a complex system. If we can approximate complex value functional with reasonable precision, we may be able to exploit system structure to a computational advantage.  
We consider separated representations of the value function $\valuefunc (\state,t)$  (c.f. \eqref{eq:lower_visc}), parameterized  by basis functions $\hilbertparam \in \hilbertparamspace$\footnote{Note that $\hilbertparamspace\subset \mathcal{F} \in \bb{R}^{d_\phi}$.} \ie $\valuefunc (\state,t; \hilbertparam)$, where, $\valuefunc (\state,t; \hilbertparam)$, defined over a region $(\omega\times \bb{R})$ ($\omega \subset \openset$), possesses real values on the Hilbert space $\mathcal{F}=H_0^1(\openset) \in \bb{R}^{d_\phi}$  that satisfy the  boundary conditions of \eqref{eq:lower_visc_boundary}. 

Let us first introduce the notations that will enable us to construct the decomposition problem. We let $\langle \cdot; \cdot \rangle_{\mc{F}}$  be the inner product associated with a norm, $\| \cdot \|_{\mc{F}}$, induced on the tensor product space $\mathcal{F} \otimes \mathcal{S}$, which we define as 
%
\begin{align}
	%&\langle \hilbertparam, \hilbertcoeff \rangle_{\mc{F}} = \int_{\mathcal{F}} \langle \hilbertparam, \hilbertcoeff  \rangle_{\mc{F}} d\state. %\\
	%
	\langle &\hilbertcoeff, \hilbertparam \rangle_{\mc{F}} = \int_{\mathcal{S}} \langle \hilbertcoeff, \hilbertparam \rangle_{\mc{F}} d\state.
	\label{eq:frob_inner_prod}
\end{align}
%
Let us introduce the function space $L^2(\mc{S}; \mc{F})$
%
\begin{align}
	L^2(\mc{S}; \mc{F}) = \left\{\hilbertparam: \mc{S} \rightarrow \mc{F}; \int_{\mc{S}} \| \hilbertparam(\state)\|^2_{\mc{F}} \,\, d\state < +\infty \right\}
\end{align}
%
and associate it with the dual space $\breve{\mc{F}} = H^{-1}(\openset)$\footnote{Following \eqref{eq:hilbert-schmidt-inner-product}, when we write a norm in the form of \eqref{eq:frob_inner_prod}, we shall mean the Frobenius inner product norm.}. \textbf{The decomposition problem is to find}
%
 $\valuefunc(\state,t; \hilbertparam) \in \mc{F} \otimes \mc{S}$ such that 
%
\begin{align}
	\dfrac{\partial \valuefunc}{\partial t}(\state, t) \in L^2(\breve{\mc{F}} \otimes \mc{S})
\end{align}
%
and 
%
\begin{align}
	\amalg(\valuefunc, \hilbertparam) = \mathbf{\uplus}(\hilbertparam), \,\, \forall \, \hilbertparam \in \mc{F} \otimes \mc{S}.
	\label{eq:bilinear_transform}
\end{align}
%
where $\amalg(\cdot, \cdot)$ and $\uplus(\cdot)$ are respectively multilinear transforms (to be introduced shortly). The solution of \eqref{eq:bilinear_transform} satisfies the boundary conditions to the initial value problem of \cf \eqref{eq:lower_hji_pde} in a weak sense~\cite{Nouy2010}. 

In this sentiment, $\valuefunc (\state,t; \hilbertparam)$ is the sum of the tensor products of free parameters  $\{\hilbertcoeff_i\}_{i=0}^\infty \in \mathcal{S}$ and basis functions $\{\hilbertparam_i\}_{i=0}^\infty \in \mathcal{F}$ \ie,   
%
\begin{subequations}
	\begin{align}
		\valuefunc(\state, t) &\approx \valuefunc(\state, t; \hilbertparam) \\
		&\equiv \sum_{i=0}^\infty \langle\hilbertcoeff_i(t), \hilbertparam_i(\bm{x})\rangle_{s}, \, \hilbertparam_i \in \mathcal{F}, \, \hilbertcoeff_i \in  \mathcal{S}
	\end{align} 
	\label{eq:decomp_general}
\end{subequations}
%
\noindent where $\valuefunc(\state, t; \hilbertparam): \ren \times \bb{R} \rightarrow \mathcal{F} \otimes \mathcal{S}$;  $\{\hilbertparam_i\}_{i=0}^\infty$ are basis functions in $\mathcal{F}$, and  $\{\{\hilbertcoeff_i\}_{i=0}^\infty  \subset \mathcal{S}\}$ represent the influence of the \textit{principal components} of $\valuefunc$ on the respective bases, $\hilbertparam_i$.  \textit{These summands over tensor products constitute the Galerkin decomposition of the viscosity solution $\valuefunc (\state,t)$ mentioned in \autoref{subsec:visc}}. 

A separable representation  of $\valuefunc(\state, t; \hilbertparam)$ of order $r$ therefore can be defined as the function
%
\begin{align}
	\valuefunc_r(\state, t) = \sum_{i=0}^{r-1} \langle \hilbertcoeff_i(t),  \hilbertparam_i(\bm{x}) \rangle_{\mc{F}}, \, \hilbertparam_i \in \mathcal{F}, \, \hilbertcoeff_i \in  \mathcal{S}
	\label{eq:decomp_reduced}
\end{align} 
%
that admits an optimal solution $\valuefunc \in  \mathcal{F} \otimes \mathcal{S}$ up to an approximation error $\epsilon>0$, established  via the Galerkin orthogonality criteria 
%
\begin{align}
	\| \valuefunc - \valuefunc_r \|^2_{F} = \min_{\substack{\{\hilbertcoeff_i\}_{i=1}^\infty \in \mathcal{S} \\ \{\hilbertparam_i\}_{i=1}^\infty \in \mathcal{F} }} \|\valuefunc - \sum_{i=0}^{r-1} \langle \hilbertcoeff_i(t), \hilbertparam_i(\state) \rangle_{\mc{F}} \|_F^2.
	\label{eq:galerk_orth_crit_reduced}
\end{align}
%
%where $\| \cdot \|_F$ is the Hilbert-Schmidt norm induced on the Frobenius inner tensor product space $\mathcal{S}\otimes \mathcal{F} = \langle \mathcal{S}, \mathcal{F}\rangle_F$ (c.f. \eqref{eq:hilbert-schmidt-inner-product}). 

%Thus, $\hilbertcoeff_i$ and $\hilbertparam_i$ can be considered optimal with respect to the  norm $\|\cdot\|_F$ induced on the Frobenius inner tensor product space $\mathcal{S} \otimes \mathcal{F}$. For the rest of this article, we will interchangeably use the notation $\mathcal{S} \otimes \mathcal{F}$ in lieu of the Frobenius inner product defined in \autoref{sec:notations}.


%For the lower HJI value $\valuefunc(\state, t)$ (c.f. \eqref{eq:value_lower}) defined on the phase space $\spatialdomain \times \timeinterval$ where $\timeinterval = (T, 0]$ is summable over the time interval $\left(T, 0\right]$, we associate $\valuefunc$  with a function defined on the interval $I$ whose values are in Hilbert space $\mathcal{V} = H_0^1(\Omega)$, and $\valuefunc(t): \state \in \openset \rightarrow \valuefunc(t) (\state) \approxeq \valuefunc(t, \state)$.

%\todo{Gently ease the reader into your jazz here. Say multilinear compositions such as tensors are now efficient way of }

Consider an environment where $\valuefunc \in \mathbb{R}^{I_0 \times I_1 \times \cdots \times I_{N-1}}$ is a high-dimensional value function for a system  of multiple interacting agents, each with dynamics $\dot{\state_0}, \, \dot{\state_1}, \, \cdots \dot{\state_n}$, and whose respective states span the full rank of each mode of $\valuefunc$. Since the value functions for such a system are of high order, we replace the value function $\valuefunc(\state, t)$ with its tensor representation,  $\mathds{V}(\state, t)$ (\cf \autoref{sec:notations}) so that  the full problem described in Lemma \ref{lemma:lower_visc_lemma}, corresponds to the following parameterized P.D.E 
%
\begin{subequations}
	\begin{align}
		 \dfrac{\partial \valuetensor}{\partial t}(\state, t; \hilbertparam) &+ \hamtensor (\state, t, p; \hilbertparam) = 0, \, t \in \left[T, 0\right],  x \in \openset, \hilbertparam \in \hilbertparamspace  \\
		%
		\valuetensor(\state, T; \hilbertparam) &= g(\state; \hilbertparam) \quad \state \in  \{ \bar{\openset} \backslash \text{int } \openset\}, \,  \hilbertparam \in \hilbertparamspace
		\label{eq:lower_visc_boundary_param}
	\end{align}
	\label{eq:lower_visc_param}
\end{subequations}
%
with corresponding Hamiltonian, 
%
\begin{align}
	\hamtensor(\state, &t, p; \hilbertparam) = \min\left\{0,\max_{\bm{u} \in \mathcal{U}} \min_{\bm{v} \in \mathcal{V}} \langle f(t; \state, \bm{u}, \bm{v}), p  \rangle \right\}.  
\end{align}
%
%The results presented in this work assume a separated representation of the value function. \todo{This separation property is valid when some parameters of the PDE becomes negligible~\cite{Nouy2010}}\cmt{This has been disproved by Vannieuwenhoven in 2010}. 
The decomposition of \eqref{eq:decomp_reduced} can be considered a pseudo-eigenvalue problem (this is established in \autoref{subsec:inc_hosvd}) %, %\cmt{although not admitting existence and convergence results}, 
which proves efficient for separated representations in many applications including stochastic nonlinear PDEs~\cite{Nouy2009, LadevezeBook} and finite element methods~\cite{NouyFEM}. The state space can be split into disjoint regions where the value function is continuously differentiable in each region. \todo{The \textit{singular surfaces}~\cite{Isaacs1965} that separate the respective disjoint value functions constitute manifolds which have discontinuous derivative properties and we follow ~\cite[Theorem 8.2]{BasarBook}'s manifold resolution strategy \ie these manifolds satisfy the saddle equilibrium strategies}
%
\begin{align}
	\hamtensor(t; \state, \control^\star, \disturb) \le \hamtensor(t; \state, \control^\star, \disturb^\star) \le \hamtensor(t; \state, \control, \disturb^\star). 
	\label{eq:saddle_ham}
\end{align}
%
\todo{When the value function is not continuously differentiable; or the value function becomes discontinuous, we resort to  classical fractional steps in finite differencing schemes for conservation laws~\cite{CrandallFractional} applied on a dimension-by-dimension basis to a mesh on which a separated composition is defined~\cite{OsherShuENO}}.
We defer the treatment of \textit{dispersal surfaces}~\cite{Isaacs1965} to a future work.

\subsection{Galerkin approximation of the Variational HJI Problem}
\label{subsec:galerkin}
%
We now derive the Galerkin approximation of the viscosity solution to the terminal HJI problem. Assume that a decomposition $V_{r}$ of order $r$ is already known (this could be obtained by a partial truncation of the value function as described in \autoref{subsec:inc_hosvd}) or randomly initialized. For the next order $r+1$, a new couple $\left(\hilbertcoeff, \hilbertparam\right)$ is optimal if it satisfies the Galerkin orthogonal metric on the induced norm of the tensor product space $\mathcal{S}\otimes \mathcal{F}$ such that 
%
\begin{align}
	\| \mathds{V} - \mathds{V}_{r} \|_F^2 &= \|\mathds{V} \|_F^2 - \sigma(\phi_i(\state)) \nonumber \\
	&\equiv \|\mathds{V} \|_F^2 - \| \valuecore \|_F^2,
	\label{eq:galerk_orth_crit}
\end{align}
%
where $\sigma(\hilbertparam_i(\state))$ denotes an eigen decomposition of $\hilbertparam_i(\state)$ and $\valuecore$ is the \textit{core tensor} of $\valuefunc$ -- representing its critical mass -- which can be obtained e.g. from a high order orthogonal iteration~\cite{Kolda2009} or from a higher order singular value decomposition~\cite{DeLathauwer2000, VannieuwenhovenTruncate2012} of $\valuefunc$ (We discuss this in \autoref{subsec:inc_hosvd}). The optimality proof of \eqref{eq:galerk_orth_crit} is given in appendix \ref{app:ortho_proj_error} via \eqref{eq:core_tensor}. 	

Suppose that the optimal or quasi-optimal\footnote{The rationale for introducing quasi-optimality is discussed in section \ref{subsec:inc_hosvd} where the Galerkin basis functions and coefficients are resolved with sequentially-truncated higher order singular value decomposition of $\mathds{V}(\state, t)$.} order $r$ for decomposing $\mathds{V}$ in \eqref{eq:lower_visc_boundary_param} has been found.
%
The quasi-optimal Galerkin-approximation of \eqref{eq:lower_visc_boundary_param} is given as
%
	\begin{align}
		\begin{split}
		-\frac{\partial}{\partial t}\left(\sum_{i=0}^{r-1} \langle \hilbertcoeff_i,  \hilbertparam_i \rangle_{\mc{F}} \right) &= 
		\min \left\{0,  
			\max_{\control \in \mathcal{U}} \, \min_{\disturb \in \mathcal{V}} \left\langle f(t; x, \control, \disturb), \right. \right. \\
		 & \qquad \qquad \quad \left. \left. \dfrac{\partial }{\partial \state}\left(\sum_{i=0}^{r-1} \langle \psi_i, \phi_i\rangle_{\mc{F}} \right) \right\rangle \right\} 
		 \end{split}
		\end{align}
	%
	\begin{align}
		\begin{split}
			-\sum_{i=0}^{r-1} \langle \hilbertcoeff_i, \dfrac{\partial \hilbertparam_i}{\partial t} \rangle_{\mc{F}}	&= \min \left\{0,  
			\max_{\control \in \mathcal{U}} \, \min_{\disturb \in \mathcal{V}} \left\langle f(t; \state, \control, \disturb), \right. \right. \\
			& \qquad \qquad \quad \left. \left. \sum_{i=0}^{r-1} \left\langle \hilbertcoeff_i, \, \dfrac{\partial \hilbertparam_i}{\partial \state} \right\rangle_{\mc{F}} \right\rangle \right\}.
		\end{split}
	\label{eq:galerk_decomp}
	\end{align}
%
Let us vectorize the coefficients $\{\hilbertcoeff\}_{i=0}^{r-1} \subset \mc{S}$ and basis functions in the separable Hilbert space $\{\hilbertparam\}_{i=0}^{r-1} \subset \mc{F}$ as follows 
	%
	\begin{align}
		\hilbertcoeffspace_r^T &= \left[\hilbertcoeff_0, \hilbertcoeff_1, \cdots, \hilbertcoeff_{r-1}\right], \,\,
		%
		\hilbertparamspace_r = \left[\hilbertparam_0, \hilbertparam_1, \cdots, \hilbertparam_{r-1}\right]^T.
	\end{align}
%
Thus, we can rewrite \eqref{eq:galerk_decomp} as 
	%
	\begin{align}
		-\hilbertcoeffspace_r^T \, \dot{\hilbertparamspace}_r  &= \min \left\{0, \max_{\control \in \mathcal{U}} \, \min_{\disturb \in \mathcal{V}} \left\langle f(t; \state, \control, \disturb), \langle \hilbertcoeffspace_r^T, \, \hilbertparamspace_{\state}\rangle_{\mc{F}} \right\rangle \right\}. %\nonumber \\
		%
		% &= \min \left\{0, \Psi_r^T \cdot \left[\max_{\control \in \mathcal{U}} \, \min_{\disturb \in \mathcal{V}} \left\langle f(t; \state, \control, \disturb), \Phi_{\state} \right\rangle_F \right] \right\}.
		\label{eq:galerk_opt}
	\end{align}
%
Note that $\hilbertparamspace_{\state}$ are the spatial derivatives of $\{\hilbertparam\}_{i=0}^{r-1}$ w.r.t $\state$. Furthermore, we choose not to factorize the coefficients $\hilbertcoeffspace_r^T$ in \eqref{eq:galerk_opt} because we want to retain the characteristics of the original value function $\mathds{V}(\state, t)$ on the respective bases, $\left\{\langle\hilbertcoeff_i, \hilbertparam_i\rangle\right\}_{i=0}^{r}$. At once, we see that if the optimal decomposition components $\{\hilbertcoeff_i\}_{i=1}^r \in \hilbertcoeffspace$ and $\{\hilbertparam_i\}_{i=0}^{r-1} \in \hilbertparamspace$ are known, equation \eqref{eq:galerk_opt} admits separable solutions  on finite meshes, rendering solution of the viscosity problem \eqref{eq:lower_hji_pde} easily computable  with the usual high precision and accuracy that Lax-Friedrichs schemes afford~\cite{CrandallLaxFriedrichs, CrandallFractional, Crandall1984, OsherShuENO}. We defer the identification of the parameters $\Phi$, $\Psi$ to \autoref{subsec:inc_hosvd}.

\subsection{Galerkin HJI Approximation Under Separable Dynamics}

Now, suppose that the dynamics $f(t; \state, \control, \disturb)$ from \eqref{eq:sys_dyn} is separable into its state, control, and disturbance components in an additive manner as follows,
%
\begin{align}
	\dot{\state} &= f(t; \state, \control, \disturb) \\
				 &= f(t; {\state}) \state(t) + f(t; \control) \control(t) + f(t; \disturb) \disturb(t),
	\label{eq:separable_dyna}
\end{align}
%
where $f (t; \state), f(t; \control),$ and $f(t; \disturb)$ are the respective components of the system dynamics for the state, control law, and disturbance. This separable dynamics is typically observed for autonomous systems such as Dubins vehicles in relative coordinates, quadcopters, and many natural systems\footnote{Even when the separation in \eqref{eq:separable_dyna} is not possible globally, we can consider a perturbation $\delta \state$ about the state $\state$ along a nominal trajectory $\bar{\state}$ so that the system's locally linear state is iteratively measured with respect to $\state$ as it is commonly done in linear quadratic methods. We defer the treatment of these locally linearized dynamics to a future work. For an in-depth treatment, see \cite{DenhamDDP, Mitter1966, McReynolds1967, Jacobson1968new}.}.

Putting \eqref{eq:separable_dyna} into \eqref{eq:galerk_opt}, we find that
%
\begin{align}
	\begin{split}
		-\hilbertcoeffspace_r^T \, \dot{\hilbertparamspace}_r  &= \min \left\{0, \max_{\control \in \mathcal{U}} \, \min_{\disturb \in \mathcal{V}} \left\langle f_x x(t) + f_u (t) \control(t), f_v \disturb(t), \right. \right. \\
		& \qquad \qquad \quad \left. \left. \left\langle \hilbertcoeffspace_r^T, \, \hilbertparamspace_{\state}\right\rangle_{\mc{F}} \right\rangle \right\},
	\end{split}
\end{align}
%
so that the term on the right hand side becomes
%
\begin{align}
	\begin{split}
		\min \left\{0,\left\langle f_x \state(t),  \left\langle \hilbertcoeffspace_r^T, \, \hilbertparamspace_{\state}\right\rangle_{\mc{F}} \right\rangle
		%
		+ \max_{\control \in \mathcal{U}} \, \left\langle f_u (t) \control(t), \right. \right. \\ 
		\left. \left. \left\langle \hilbertcoeffspace_r^T, \, \hilbertparamspace_{\state}\right\rangle_{\mc{F}} \right\rangle 
		%
		+ \min_{\disturb \in \mathcal{V}}\, \left\langle   f_v \disturb(t), \left\langle \hilbertcoeffspace_r^T, \, \hilbertparamspace_{\state}\right\rangle_{\mc{F}} \right\rangle \right\}.
	\end{split}
	  \label{eq:galerk_separable}
\end{align}

The attractiveness of \eqref{eq:galerk_separable} is that the respective Hamiltonians can be parallelized on multiple cores during iterations of the decomposition and then assembled on a centralized node to accelerate computation with for example, the alternating direction method of multipliers~\cite{BoydADMM}. \todo{This is treated and an example is given in \autoref{sec:admm}} 

Similar to \eqref{eq:galerk_opt}, \eqref{eq:galerk_separable} can be resolved on a mesh. However, when the dynamics are separable as in the foregoing, the saddle point necessary condition \ie \eqref{eq:saddle_points} allows us to find an analytic solution. \todo{We defer this treatment to a future work as follows:}
%
\todo{Under development. A future paper?}

\subsection{Galerkin HJI Approximation Under $H$-$\infty$ Worst Disturbance Control}
\label{sec:hinfty}

\todo{Under Development}


\subsection{PGD Decomposition Scheme}%High-Order Quasi-Optimal Tensor Decompositions}
%
%The authors of~\cite{DecompChenHerbert} proposed a  local scheme that decomposed BRS and BRTs by the union and intersection of points on grid substructures, for basic geometric primitives, these projection and back-projections work reasonably well. For higher-order systems, where there is coupling among the subsystem modes and with BRS/BRTs whose geometry are nonlinear, low-rank approximations of tensor-structured BRS/BRTs can provide reasoned state relationships between decomposed structure and the global BRTs. %For high-dimensional systems, decomposition of reachable sets and tubes seem reasonable given recent work that explores this locally~\cite{DecompChenHerbert}.

%\begin{figure}[tb!]
%	\centering 
%	\includegraphics[width=\columnwidth]{figures/cylinder_2d.jpg}
%	\caption{Implicit representation of a value function defined as a signed distance function on a grid's interface.}
%	\label{fig:value_cyl}
%\end{figure}
%
%\begin{figure}[tb!]
%	\centering 
%	\includegraphics[width=\columnwidth]{figures/rect4_2d.jpg}
%	\caption{Implicit representation of a value function defined as a signed distance from grid points to an interface specified on the grid.}
%	\label{fig:value_rect}
%\end{figure}

%$\mathds{V}$,  implicitly defined as a signed distance to points on the grid on the left of \autoref{fig:value_rect}. The zero level set of this value function is  the right inset. A BRS/BRT defined by this hyperrectangle only span a finite rank of the state space (the mesh) on which $\mathds{V}$) is defined. 

It now remains for us to establish an optimal way to compute the basis and coefficients of the optimal Galerkin decomposition in \eqref{eq:galerk_opt} and \eqref{eq:galerk_separable}. \todo{First, we introduce the following multilinear mappings:
	%
	\begin{itemize}
		\item Let $S_r: \mc{F}  \rightarrow \mc{S}$ be the multilinear transformation from a real-valued state in $\phi \in \mc{F}$ to $\psi \in \mc{S}$ \ie,
		\begin{align}
			\amalg(\valuefunc_{r-1}+\phi \psi, \phi \psi_1^\star + \phi_1^\star \psi,\psi_2^\star \cdots \, \psi_r^\star) &= \uplus(\phi^\star \psi),\\
			&\qquad \forall \, \phi^\star \in \mc{F}.
		\end{align}
		%
		\item Let $F_r: \mc{S}  \rightarrow \mc{F}$ be the multilinear transformation from  $\hilbertcoeff \in \mc{S}$ to $\hilbertparam \in \mc{F}$ \ie,
		\begin{align}
			\amalg(\valuefunc_{r-1}+\phi \psi_0, \phi \psi_1, \cdots \phi \, \psi_r) = \uplus(\phi^\star \psi), \forall \, \phi^\star \in \mc{F}.
		\end{align}
	\end{itemize}
}
%
\todo{we establish a Lemma due to~\cite{DeLathauwer2000} that allows every tensor $\mathds{V}$ to admit a higher-order singular value decomposition}. In our treatment, we resort to higher-order singular value decomposition (HOSVD)~\cite{Tucker66}\footnote{In his original work, Tucker only prescribed the decomposition of a tensor for up to 3 modes.}, extended to $N$-way tensors by~\cite{KapteynNWayTensors1986}. This consists in decomposing it into the product of a core tensor, $\valuecore \in \mathbb{R}^{R_0 \times R_1 \times \cdots \times R_n}, \, (R_n \le I_n)$, and \textit{unit norm factor matrices}, $\bm{U}_{n=0}^{N-1}$ of size $I_n \times R_n$ are of  $\mathds{V}$ (where $\valuetensor \in \mathbb{R}^{I_0 \times I_1 \times \cdots \times I_n}$)
%
\begin{subequations}
	\begin{align}
		\mathds{V}  &\approx \mathds{\hat{V}}  \\
		&= \valuecore \otimes_0 \mathbf{U}_0 \otimes_1 \mathbf{U}_1 \otimes_2 \mathbf{U}_2 \cdots \otimes_{N-2} \mathbf{U}_{N-1}.
	\end{align}
	\label{eq:tucker}
\end{subequations}
%
\noindent The matrices $\mathbf{U}_0 \in \mathbb{R}^{I_0 \times R_0}, \, \mathbf{U}_1 \in \mathbb{R}^{I_1 \times R_1}, \cdots \mathbf{U}_{N-1}$ can be seen as representing the influence of the principal components of each mode of $\mathds{V}$ on the \textit{core tensor} $\valuecore$.  Put differently, in dynamical systems parlance, this can be seen as the influence of a subsystem agent's dynamics (on a value function subspace)  on the overall value function of all interacting agents. The entries of $\valuecore$ denotes level of interaction between the different components $\bm{U}_n$. They can be thought of as the critical mass of the system's interaction -- encoding the objectives of the separate dynamical systems that share a large cyberphysical system space. The decomposition outlined in \eqref{eq:tucker} can be obtained via the following minimization problem
%
\begin{align}
	\min_{\valuecore,\, \mathbf{U}_0, \cdots, \mathbf{U}_{N-1}} &\| \valuetensor
	 - \valuecore \otimes_0 \mathbf{U}_0 \cdots \otimes_{N-2} \mathbf{U}_{N-1} \|^2_2 \nonumber\\
	 \text{subject to } &\valuecore \in \bb{R}^{R_0 \times R_1 \cdots \times R_{n-1}} \nonumber \\
	\text{ and orthonormal } &\{\mathbf{U}\}_{n=0}^{N-1} \in \bb{R}^{I_n\times R_n}. 
	 \label{min:tucker_decomp}
\end{align}
%
%The minimization problem in \ref{min:tucker_decomp} can be resolved efficiently with \autoref{alg:tucker_power_iter} -- an adaptation of 

\begin{algorithm}[tb!]
	\caption{Value Function Decomposition  
		\label{alg:tucker_power_iter}}
	\begin{algorithmic}[1]
		\Function{ValuePower}{$\epsilon$}
		\Comment{Fix $\epsilon$, convergence threshold.}
		%
		\State Initialize $\{U\}_{n=0}^{n=N-1}$ \label{line:left_dom_eigvec}
		\Comment{$\valuetensor_n$ left dominant single vecs.} 
		%
		\State Set $\delta = +\infty$
		\Comment{$\delta$: Least-squares fit quality.}
		%
		\While{$\delta  > \epsilon$}
		\For{$n=0, \cdots, N-1$}
		\State $\mathbf{Q}_n \leftarrow \valuecore \otimes_0 \mathbf{U}_0^T \otimes_1 \mathbf{U}_1^T \cdots \otimes_{N-1} \mathbf{U}_{N-1}^T$
		\State $\mathbf{U}_n \leftarrow \frac{\mathbf{Q}_n}{\|\mathbf{Q}_n\|_2}$
		\EndFor
		\State $\delta \leftarrow \|\mathbf{U}_n^T - \mathbf{U}_{n-1}^T\|_2$
		\Comment{Convergence check.}
		\EndWhile
		\State $\valuecore \leftarrow \valuetensor \otimes_0 \mathbf{U}_0^T \otimes_1 \mathbf{U}_1^T \cdots \otimes_{N-1} \mathbf{U}_{N-1}^T$. \label{tucker_decomp:line_value_core}
		\State \Return{$\valuecore, \{\mathbf{U}_n\}_{n=0}^{N-1}$}
		\EndFunction
	\end{algorithmic}
\end{algorithm}

An efficient means for finding the factor matrices of minimization problem in \ref{min:tucker_decomp}  is a power iterations algorithm similar to DeLathauwer et. al's \cite{DeLathauwer2000}. A variation of DeLathauwer et. al's algorithm for our problem is presented in Algorithm \autoref{alg:tucker_power_iter}. On line \autoref{line:left_dom_eigvec}, the left dominant singular vector of $\valuetensor$ can be found via Tucker's ``Method I" for computing the rank-$\left(R_0, R_1, \cdots, R_{N-1}\right)$ decomposition and we refer readers to~\cite{Tucker66}. If time to find the orthonormal factor matrices is not of large concern, they can be randomly initialized as well.

On line \autoref{tucker_decomp:line_value_core} of \autoref{alg:tucker_power_iter}, using the orthonormal components of \eqref{eq:tucker}, we compute the \textit{optimal core} $\valuecore$ of $\mathds{V}$  (we refer readers to \cite{DeLathauwer2000} for a derivation) as 
%
\begin{align}
		\valuecore  &= \mathds{V} \otimes_0 \mathbf{U}_0^T \otimes_1 \mathbf{U}_1^T \otimes_2 \mathbf{U}_2^T \cdots \otimes_{N-2} \mathbf{U}_{N-1}^T.
		\label{eq:core_tensor}
\end{align}
%
Substituting \eqref{eq:core_tensor} into \eqref{eq:tucker}, we find that the approximation of $\mathds{V}$ is  a projection of the original value function tensor along its respective $n$-basis modes onto the reduced system, i.e.
%
\begin{align}
	\mathds{\hat{V}}  &= \valuetensor \otimes_0 \mathbf{U}_0 \mathbf{U}_0^T    \cdots \otimes_{N-2} \mathbf{U}_{N-1} \mathbf{U}_{N-1}^T. 
	\label{eq:value_reduced}
\end{align}
%
Therefore, the approximation error is $\|\valuetensor - \mathds{\hat{V}}\|^2 = \|\mathds{V}\|^2 - \|\valuecore\|^2$ (as shown in Appendix \ref{app:ortho_proj_error}). 

In light of our Galerkin approximations and the validity of any resulting BRS or BRT approximation, since we work with a truncated decomposition of $\valuefunc$ at each step of the algorithm. These truncations correspond to the  decomposed value function on various subspaces of the system. Because we keep the nonanticipative  strategies of $\pursuer$, the reachable set is still overapproximated in the larger sense, whereupon the pursuer makes decisions about $\disturb$ with full knowledge of $\control(\tau)$ for $\tau \in \left[t, t_f\right]$. Within this tolerance, we want to ensure  A truncated decomposition up to mode-$r$ would consist of a \textit{partial core} $\valuecore_r$ and its corresponding orthonormal matrices $\bm{U}_0, \cdots, \bm{U}_r$, $r \in \left[R \right]$ defined as 
%
\begin{align}
	\valuecore_r = \mathds{V} \otimes_0  \mathbf{U}_0^T \otimes_1 \mathbf{U}_1^T \otimes_2 \mathbf{U}_2^T \cdots \otimes_{r-1} \mathbf{U}_{r}^T,
	\label{eq:partial_core}
\end{align}
%
so that the truncated value function (up to mode $r$) is 
%
\begin{subequations}
	\begin{align}
		\valuetensor_r &= \valuecore_r \otimes_0 \mathbf{U}_0 \otimes_1 \mathbf{U}_1 \otimes_2 \mathbf{U}_2 \cdots \otimes_{r-1} \mathbf{U}_{r} \label{eq:value_trunc} \\
		%
		&\equiv  \mathds{V} \otimes_0  \mathbf{U}_0\mathbf{U}_0^T \otimes_1  \cdots \otimes_{r-1} \mathbf{U}_{r}\mathbf{U}_r^T, \, r \in \left[R\right]
		\label{eq:tucker_trunc} 
	\end{align}
\end{subequations}
%
where \eqref{eq:tucker_trunc} is a result of putting \eqref{eq:partial_core} into \eqref{eq:value_trunc}.  

Now, revisiting the functions $\{\hilbertcoeff\}_{i=0}^{r-1}$ and $\{\hilbertparam\}_{i=0}^{r-1}$ in the previous two sections, a convenient way to compute the coefficients and basis functions that satisfy the Galerkin orthogonality criterium \eqref{eq:galerk_orth_crit} is to set 
%
\begin{align}
	\hilbertcoeff_r = \valuetensor \otimes_0 \mathbf{U}_0^T \cdots \otimes_{r-1} \mathbf{U}_r^T
	\label{eq:hilbert_coeff_compute}
\end{align}
%
and 
%
\begin{align}
	\hilbertparam_r = \valuecore_{r} \otimes_0 \mathbf{U}_0 \cdots \otimes_{r-1} \mathbf{U}_r,
	\label{eq:hilbert_basis_compute}
\end{align}
%
where again, $\mathbf{U}_0, \cdots \mathbf{U}_{N-1}$ are factor matrices obtained from Algorithm \ref{alg:tucker_power_iter}.

Algorithm \ref{alg:pgd-power} describes how we  compose the separable value function that satisfies \eqref{eq:bilinear_transform}. Lines \ref{algpgd:line_coeff} and \ref{algpgd:line_basis} 
describe the optimal resolution of the decomposition parameters, $\{\hilbertcoeff\}_{i=0}^{r-1}$, and basis functions, $\{\hilbertparam\}_{i=0}^{r-1}$. In line \ref{algpgd:line_hji_update} of the algorithm, the integral is solved using Total Variation Diminishing (TVD) Runge-Kutta scheme as described in \cite[\S3.5]{LevelSetsBook} (originally implemented in \cite{MitchellLSToolbox2007}, which we re-implement in CuPy~\cite{CuPy} as we leverage parallel computation). In addition, having $r$ as an input variable into the algorithm allows us to take advantage of warm-starting schemes, typical in Reinforcement learning schemes~\cite{FisacICRA} so that the algorithm need not be run in one fell-swoop. Partial BRT's and BRS's can be distributively learned on separate CPU/GPU cores, and later assembled on a centralized nodes to aid faster computation. \todo{To be developed}.
This may for example be similar to the alternating direction method of multipliers.

\begin{algorithm}[tb!]
	\caption{Iterative Scheme for Computing BRS/BRTs  
		\label{alg:pgd-power}}
	\begin{algorithmic}[1]
		\Function{IterativeBRT}{$\valuetensor_{r-1}, r_{max}$}
		\Comment{$r_{max}$: max. iter.}
		%
		%\State Set $\valuetensor_{r-1}= \langle \psi_{0}, \phi_{0}\rangle$
		%
		\While{$r  < r_{max}$}
		\State Compute $\hilbertcoeff_r$ from \eqref{eq:hilbert_coeff_compute}. % $ \valuetensor_{r-1} \otimes_0 \mathbf{U}_0^T \cdots \otimes_{r-1} \mathbf{U}_r^T$
		\label{algpgd:line_coeff}
		\State Compute $\hilbertparam_r$ from \eqref{eq:hilbert_basis_compute}. %$ \valuecore_{r-1} \otimes_0 \mathbf{U}_0 \cdots \otimes_{r-1} \mathbf{U}_r$
		\label{algpgd:line_basis}
		\State Set $\valuetensor_r \leftarrow  \valuetensor_{r-1} + \langle\psi_r,\, \phi_r \rangle_{\mc{F}}$
		\Comment{Update $\valuetensor_r$.}
		%
		\State Set $\langle \hilbertcoeffspace_r^T, \, \dot{\hilbertparamspace}_r \rangle_F \leftarrow \valuetensor_r$  \cf \eqref{eq:galerk_opt} or  \eqref{eq:galerk_separable}
		\label{algpgd:line_hji_update}
		%
		\State $\mathds{L}_r \leftarrow \int_{\mathcal{S}} \langle \hilbertcoeffspace_r^T, \, \dot{\hilbertparamspace}_r \rangle_{\mc{F}} d\state$. 
		\Comment{Partial BRS(T), $\mathds{L}_r$}
		%
		\State $r = r+1$.
		\Comment{Advance the decomposition.}
		\EndWhile
		\State \Return{$\{\mathds{L}\}_{r=0}^{r_{max}-1}$}
		\EndFunction
	\end{algorithmic}
\end{algorithm}

Once the respective backward reachable tubes are collected, we must stitch them together such that their composition satisfies the boundary condition of the initial value problem of \eqref{eq:lower_hji_pde}. \todo{Lekan: Discuss the treatments of the singular and dispersal surfaces here.}

In algorithm \ref{alg:pgd-power}, the maximum number of iterations, $r_{max}$ can be chosen in an informed way as highlighted below. First, we restate the following theorem  that allows us to provide a bound on the projection error.
%
\begin{theorem}{[Vannieuwenhoven, Vandebril, and Meerbergen]}\cite[Th. 5.1]{VannieuwenhovenTruncate2012}. 
	Suppose $\mathds{V}$ is a tensor of size $I_0 \times I_1 \times \cdots \times I_{N-1}$, approximated by $\mathds{\hat{V}}$ as in \eqref{eq:value_reduced}, the approximation error of \eqref{eq:galerk_orth_crit} is 
	%
	\begin{align}
		\|\mathds{V} &- \mathds{\hat{V}} \|^2_F = \| \mathds{V} \otimes_0 \left( \mathbf{I} - \mathbf{U}_0 \mathbf{U}_0^T \right)\|_F^2 \nonumber \\
		& \quad + \| \mathds{\tilde{V}}_0 \otimes_1 \left( \mathbf{I} - \mathbf{U}_1 \mathbf{U}_1^T \right)\|_F^2 + \cdots\nonumber \\
		& \,\,    + \| \mathds{\tilde{V}}_{N-2} \otimes_{N-1} \left( \mathbf{I} - \mathbf{U}_{N-1} \mathbf{U}_{N-1}^T \right)\|_F^2.
	\end{align}
	\label{theorem:hosvd}
\end{theorem}
%
Furthermore, the approximation error is bounded by 
%
\begin{align}
	\| \mathds{V} - \mathds{\hat{V}} \|_F^2 \le \sum_{n = 0}^{N-1} \| \mathds{V} \otimes_n \left(\mathbf{I} - \mathbf{U}_n \mathbf{U}_n^T\right)\|_F^2.
	\label{eq:approx_error}
\end{align}
%
Equation \eqref{eq:approx_error} allows us to choose an approximation error that informs us about the level of information we want preserved on the decomposed value $\mathds{V}$. Therefore, to  find a Tucker decomposition $\mathds{\hat{V}}$ of $\mathds{V}$ whose relative decomposition error is no greater than a certain positive $\epsilon$, we first unfold the tensor along one of its modes, compute the Gram matrix and then do an eigen decomposition:
%
\begin{align}
	\mathbf{G} \equiv \mathds{V}_{(n)} \, \mathds{V}_{(n)}^T = \mathbf{V} \mathbf{\Lambda} \mathbf{V}^T,
\end{align}
%
where $\mathbf{\Lambda} = \text{diag}\left(\{\lambda_1, \lambda_2, \cdots, \lambda_{I_n}\}\right)$, and $\lambda_1 \ge \lambda_2 \ge \cdots > \lambda_{I_n} \ge 0$, and $\mathbf{V}$ contains the corresponding eigenvectors. Therefore, we can choose the orthonormal matrix $\mathbf{U}_n$ and the low-rank tensor  $R_{n-1}$ at mode $n-1$ as 
%
\begin{align}
	\mathbf{U}_n = \mathbf{V}\left[:,0:R_{n-1}\right]  
\end{align}
%
where  
%
\begin{align}
	R_n &= \min_{R \in \left[ I_n \right]} R \nonumber \\
	\text{ subject to } &\sum_{i=R+1}^{I_n} \lambda_i \le \epsilon^2 \|\mathds{V}\|^2/N.
\end{align}

We can iteratively choose $R_n$ from $n=1$ up to an $n=r$ that guarantees we satisfy the error bound in  \eqref{eq:approx_error} so that
%
\begin{align}
	\| \mathds{V} \otimes_{n} \left(\mathbf{I} - \mathbf{U}_n \mathbf{U}_n^T\right) \|_F^2 \le \epsilon^2 \frac{ \| \mathds{V}\|_F^2}{N}.
\end{align}
%


Whilst appealing, algorithm \ref{alg:pgd-power} utilizes the full value function at each step (\cf \eqref{eq:hilbert_coeff_compute}); therefore it does not easily lend itself to large-scale problems. In what follows, we propose an incrementally-constructed value function whereupon we work with the \textit{partial cores} of $\valuefunc$ at each step of the PGD iteration. In essence, we construct the next factor matrix based on the previously computed value core. 


\subsection{Informed Incremental Value Function Decompositions}
\label{subsec:inc_hosvd}

Next, we will leverage the sequentially-truncated high-order SVD of \cite{VannieuwenhovenTruncate2012} to device an iteratively refined decomposition scheme onto which we will project the high-order value function. Working with partial cores, at step $n$, we generate the next factor matrix based on $\mathds{\hat{V}}_{n-1}$. In particular, if the conditions of Theorem \ref{theorem:hosvd} hold, then
%
\begin{align}
	\|\mathds{V} - \mathds{\hat{V}}\|_F^2 &=  \| \mathds{V} \otimes_0 \left( \mathbf{I} - \mathbf{U}_0 \mathbf{U}_0^T \right)\|_F^2 +  \| \mathds{\tilde{G}}_0 \otimes_1   \nonumber \\
	& \, \left( \mathbf{I} - \mathbf{U}_1 \mathbf{U}_1^T \right)\|_F^2 \cdots  + \| \mathds{\tilde{G}}_{N-2} \otimes_{N-1} \nonumber \\
	& \, \left( \mathbf{I} - \mathbf{U}_{N-1} \mathbf{U}_{N-1}^T \right)\|_F^2.
\end{align}
%
For convenience, we reproduce algorithm I of \cite{VannieuwenhovenTruncate2012} in Algorithm \ref{alg:st-hosvd}.

\begin{savenotes}
	\begin{algorithm}[tb!]
		\caption{Incrementally Truncated Value Function\cite{TuckerMPI}
			.}
		\label{alg:st-hosvd}
		\begin{algorithmic}[1]
			\Function{TruncatedValue}{$\mathds{V}, \, \epsilon$}
			\Comment{%$\mathds{V}$: Full value function; 
				$\epsilon$: Desired accuracy.}
			%
			\State $\mathds{P} \leftarrow \mathds{V}$ 
			\Comment{$\mathds{P}:$ c.f. \eqref{eq:ttm}.}
			%			
			\For{$n=0, 1, \cdots, N-1$}
			\State$\mathbf{G} \leftarrow \mathds{P}_{(n)} \mathds{P}_{(n)}^T$
			\Comment{$\mathbf{G}$: Gram matrix.}
			%
			\State$(\lambda, \mathbf{W}) \leftarrow \text{eig } (\mathbf{G})$
			\Comment{eig: Eigen Decomposition.}
			%
			\State $R_{n} \leftarrow \min { R \in \left[ I_{n} \right] | \sum_{i=R+1}^{I_n} \lambda_i \le \epsilon^2 \|\mathds{V}\|^2 / N }$
			\State $\mathbf{U}_n \leftarrow \mathbf{W}\left[:,0:R_{n-1}\right]$
			\Comment{Orthonormal $\mathbf{U}_n$}
			\State $\mathds{P} \leftarrow \mathbf{G} \otimes_n \mathbf{U}_n^T$
			\EndFor
			\State $\valuecore \leftarrow \mathds{P}$ 
			\Comment{$\valuecore$: Update core tensor.}
			%
			\State $\mathbf{U} \leftarrow \{\mathbf{U}_0, \cdots, U_{N-1} \}$
			\State \Return $(\valuecore, \mathbf{U})$
			\Comment{$\valuecore$: Core tensor; $\{\mathbf{U}\}_{i=0}^{N-1}$: Orthonomal matrices.}
			\EndFunction
		\end{algorithmic}
	\end{algorithm}
\end{savenotes}



\begin{comment}
At issue in our optimal analytic framework is the construction of \textit{a priori}  separated representations without knowing the solution or an approximation to the reachability problem. Ladev\`{e}ze's LArge Time INcrement (LATIN) nonlinear iterative solver~\cite{LadevezeBook} improves the memory and computational times for solving multiple linear evolution problems in global time. By robustly addressing history-dependent mechanical properties of equations, a domain decomposition scheme first separates a mechanical structure into substructures and their boundaries that are spatially local and possibly with nonlinear equations; an iterative and parallel scheme then generates kinematic admissibility for its equilibrium computations that are linear and possibly global in space variables. %  of local and nonlinear dynamics from global and linear steady state phenomena whereupon a spatio-temporal separation 
This allows important savings in computational time and problem size~\cite{BlanzeModular, ChampaneyLarge}.


%Each substructure is considered as a structure on its own and communicates only with its neighbouring interfaces. The local nonlinearities are treated in a local and mixed manner through a constitutive law associated with an interface. Thus, interfaces are bidimensional entities with their own behaviour and their own associated unknowns. An iterative scheme leads to the resolution of independent problems on each substructure and independent nonlinear mixed problems on the interfaces. This domain decomposition method also introduces some modularity when considering local nonlinearities through suited interfaces which easily model the technological reality (prestresses, gaps, unilateral contact, friction, rubber joints, etc.). ~\cite{ChampaneyLarge}


%We know that the optimality condition of the dynamic programming value in differential control theory implies that the value is the viscosity solution of the corresponding Hamilton-Jacobi-Bellman PDE~\cite{Lions1982}, and 

%
\begin{align}
H(x, u(x), Du(x)) = 0 \quad \text{ for } x \in \openset, \, u=z \text{ on } \partial \openset
\label{eq:dyna}
\end{align}
%
where $\openset$ is an open set in $\mathbb{R}^n$, and $u$ and $z$ are on the boundary of the open set \ie $\partial \openset$. For problems where the dynamics explicitly depend on time, we will pose the initial value problem for \eqref{eq:dyna} as the \textit{Cauchy problem} HJ equations, \ie,
%
\begin{align}
u_t & + H(x(t), t, u(x(t)), Du(x(t))) = 0,   \, \openset \times \left[0, T\right], \nonumber \\
u&=z \text{ on } \partial \openset \times \left(0, T\right], \quad u(x, 0) = u_0(x), \quad \in  \openset
\end{align}
%
where  $z$ and $u_0$ are known functions on the boundary and the flow field (or Hamiltonian)  $H(\cdot)$ is the mapping:  $H:\openset\times \mathbb{R}^m\times \reline^n \rightarrow \reline$, and  $Du$ is the spatial gradient of $u$ with respect to $x$
%
\begin{align}
Du = \left(\frac{\partial u}{\partial x_1}, \cdots, \frac{\partial u}{\partial x_n} \right).
\end{align}


A key insight into the  robustly optimal feasible solutions of trajectories in high dimensional state spaces is by characterized by extending Theorem 1.3 of \cite{Crandall1984}. Basically, we let a cell $\bar{\mathcal{X}} \subset \mathcal{X}$ within the partition space of the state be divisible into a union of two subsets  piecewise $C^1$ viscosity solutions of $H=0$ by dividing the open set, $\openset$, by a $C^1$ surface $\Gamma: \openset = \openset_{+} \cup \openset_{-} \cup \Gamma$ into two open subsets, $\openset_{+}$ and $\openset_{-}$]in the following theorem,
%
\begin{theorem}
Let $u \in C(\openset)$ and $u \in u_{+} \in \openset_{+} \cup \Gamma$, $u = u_{-} \cup \Gamma$ where $u_+$ and $u_{-} $ are class $C^1 \in \openset_{+} \cup \Gamma$, and $\openset_{-} \cup \Gamma$. Then $u$ is a viscosity solution of \eqref{eq:dyna} if the following holds:
\begin{enumerate}[(i)]
\item $u_{+}$ and $u_{-}$ are classical solutions of $H=0$ in $\openset_{+}$ and $\openset_{-}$ respectively, and 
%
\item if $x_0 \in \Gamma$, $Tx_0 = \{\tau \in \bb{R}^m: n(x_0) \cdot \tau = 0\}$ is the tangent space to $\Gamma$ at $x_0$ and $P_T$ is the orthogonal projection of $\bb{R}^m$ onto %Tx_0$, then 
\end{enumerate}
\label{th:viscosity_divide}
\end{theorem}

 
%\subsection{PGD Description}

\noindent Without loss of generality, we here describe an incremental proper generalized decomposition scheme on a three-dimensional Value function (where the 2D case is a degenerate). Denote by $\Psi_m = \{\Psi_i\}_{i-1}^m \in \left(\Xi\right)^m$ the set of space functions and $\Phi_m = \{\Phi_i\}_{i-1}^m \in \left(\mathcal{T}\right)^m$ the set of time functions of $u_m = \Psi_m \cdot \Phi_m$ in \eqref{eq:decomp_general}.

We start with a substructure of the state space, $u_{m-1}$ which is assumed to be known. A new residual $(\phi \times \psi) \in \Xi \times \mathcal{T}$ is an optimal couple that satisfies the double Galerkin orthogonality criterium, 
%
\begin{align}
	B(V^-_{m-1} + \psi \phi, \psi \phi^\star + \psi^\star \phi) &= L(\psi \phi^\star +  \phi^\star \psi), \nonumber \\
	&\qquad \ \forall \, \phi^\star \in \mathcal{T}, \,\, \forall \, \psi^\star \in \Xi
\end{align}

We consider the following space to time and time to space functional mappings
\begin{definition}
    $S_m: \mathcal{T} \rightarrow \Xi$ is the mapping that associates a function $\phi \in \mathcal{T}$, into a space function $\psi = S_m(\phi) \in \Xi$, given as
    \begin{align}
        B(V^-_{m-1} + \psi \phi, \psi^\star \phi) = L(\psi^\star, \phi), \quad \psi^\star \in \Xi.
    \end{align}
\end{definition}
%
\begin{definition}
    $T_m: \Xi \rightarrow  \mathcal{T} $ is the mapping that associates a function $\psi \in \Xi$, into a time function $\phi = \mathcal{T}_m(\psi) \in \mathcal{T}$,  so that
    %
    \begin{align}
        B(V^-_{m-1} + \psi \phi, \phi^\star \psi) = L(\phi^\star, \psi), \quad \phi^\star \in \mathcal{T}.
    \end{align}
\end{definition}
%
\end{comment}

