\section{Decomposition Scheme.}
\label{sec:methods}
 %:
\subsection{Power Series Approximation of the Terminal Cost}
\seclabel{subsec:power_series}
%
Let us first to introduce the quadratic approximation scheme of the value function on a reduced basis before discussing the learning method for the optimal Galerkin spectral decomposition. 

Suppose an (optimal) reduced basis with order $r$ has been found that admits the most energetic modes of $\valuefunc$. Call the cost on this basis $\valuefunc_r$. Define the state, control, and disturbance on the reduced basis as $\state_r(t), \, \control_r(t), \, \text{ and } \disturb_r(t)$ respectively, where $t \in \left[T, 0\right]$. When we decompose the system into the reduced  $\state_r(t), \, \control_r(t), \, \text{ and } \disturb_r(t)$, the dynamics no longer describes the original states and controls, but rather the variation of the state and controls on the reduced basis from the state and the control pairs on the nonlinear system of equation \eqref{eq:sys_dyn} \ie $\delta \state(t), \, \delta \control(t), \,\text{ and } \delta \disturb(t)$ respectively\footnote{Note that $\delta \state(t), \, \delta \control(t), \,\text{ and } \delta \disturb(t)$ are respectively measured with respect to $\state(t), \control(t), \disturb(t)$ and are not necessarily small. However, our case is very much helped when they are small and we conjecture that our decomposition scheme favors the smallness in the values of these variations.}. It follows that we can write the following relations
%
\begin{subequations}
	\begin{align}
		\state(t) &= \state_r(t) + \delta\state(t), \,\, 	\control(t) = \control_r(t) + \delta\control(t), \\
		%
		\disturb(t) &= \disturb_r(t) + \delta\disturb(t), \,\, t \in \left[-T, 0\right].
	\end{align}
	\label{eq:variations}
\end{subequations}
%
For convenience' sake, let us drop the templated time arguments in \eqref{eq:variations} so that our canonical problem becomes
%
\begin{subequations}
	\begin{align}
		\eqref{eq:sys_dyn} \implies \dfrac{d}{dt}\left(\state_r + \delta \state\right) &= f(t; \state_r + \delta \state, \control_r + \delta \control, \disturb_r + \delta \disturb), \,  \\
		& \state_r(0) + \delta \state(0) = \state(0).
	\end{align}
\end{subequations}
%
\eqref{eq:lower_visc} implies 
%
\begin{align}
	\begin{split}
		-\frac{\partial \valuefunc}{\partial t}(\state_r + \delta \state, t)
		&= 
		\min \left\{0,  
		\max_{\delta \control \in \mathcal{U}} \, \min_{\delta \disturb \in \mathcal{V}} \left\langle f(t; \state_r + \delta \state, \right. \right. \\
		&  \left. \left.  \control_r + \delta \control, \disturb_r + \delta \disturb), \dfrac{\partial \valuefunc}{\partial \state}\left(\state_r + \delta \state, t \right) \right\rangle \right\}. 
	\end{split} \nonumber \\
	%
	\eqref{eq:lower_visc_boundary} \implies	\valuefunc(\state_r, 0) &= g(0; \state_r(0) + \delta \state (0));
	\label{eq:canonical_value}
\end{align}
%
and
%
\begin{align}
	\eqref{eq:HJ_traj} \implies	\bm{\xi}(t) = \bm{\xi}(t; t_0, \state_r + \delta\state, 	\control+\delta\control,\disturb+\delta\disturb).
\end{align}
%
In particular, on the reduced order basis (ROB), the state dynamics now become
%
\begin{subequations}
	\begin{align}
		\dot{\state}_r(\tau) &= f(t; \state_r(\tau), \control_r(\tau), \disturb_r(\tau)), \quad \tau \in \left[-T, 0\right] \\
		\state_r(0) &= \state.
	\end{align}
\end{subequations}
%
Let the optimal cost  for using the optimal control $\control^\star(\tau) = \control_r(\tau) + \delta \control^\star(\tau)$ when $\tau \in \left[t, 0\right]$ on the phase $(\state_r, t)$ be denoted $\valuefunc^\star(\state_r, t)$; and the ROM cost for using $\control_r(\tau); \,\, \tau \in \left[t, 0\right]$ be $\valuefunc_r(\state_r, t)$. Suppose further that we denote  the difference between these two costs on the phase $\left(\state_r, t\right)$ by $\costdiff^\star$, then we have
%
\begin{align}
	\costdiff^\star =\valuefunc^\star(\state_r, t) -\valuefunc_r(\state_r, t).
	\label{eq:cost_diff}
\end{align}
%
\begin{theorem}
	The HJI variational inequality \cf \eqref{eq:lower_hji_pde} admits the following approximated expansion on the reduced model:
	\begin{align}
		\begin{split} 
			-\frac{\partial \valuefunc_r}{\partial t} -\frac{\partial \costdiff}{\partial t} - \left\langle \dfrac{\partial\valuefunc_{\state}}{\partial t}, \delta\state \right \rangle -  \dfrac{1}{2} \left\langle \delta \state, \dfrac{\partial \valuefunc_{\state\state} }{\partial t}\delta\state \right \rangle &=  \\
			\min \left\{\bm{0},  
			\max_{\delta \control \in \mathcal{U}} \, \min_{\delta \disturb \in \mathcal{V}} \left\langle f^T(t; \state_r + \delta \state, \control_r + \delta \control,  \disturb_r + \delta \disturb), \right. \right. \\
			\left. \left. \valuefunc_{\state} +  \valuefunc_{\state\state}\, \delta \state \right\rangle \right\}. 
		\end{split}
		\label{eq:ROM_HJI}
	\end{align}
	Furthermore, this expansion is bounded by $O(\delta \state^3)$.
	\label{th:quad_approx}
\end{theorem}
%
\begin{corollary}
	If the viscosity solution obtained via a Lax-Friedrichs integration scheme for solving \eqref{eq:ROM_HJI} converges to a local optimum, then the backward reachable tube will converge to a locally optimal solution. In addition, if we overapproximate the resulting numerical solution, the reachable set or tube will converge to an optimal region in the state space.
\end{corollary}

\begin{proof}
	The  singular value decomposition of $\valuefunc$ is 
	%
	\begin{align}
		\valuefunc = \bm{\Upsilon} \bm{\Lambda} \bm{\Uptheta}^T,
	\end{align}
	%
	where, $\bm{\Upsilon} \in \mathds{C}^{n\times r}, \, \bm{\Lambda} \in \mathds{C}^{r\times r}, \, \bm{\Uptheta} \in \mathds{C}^{m\times r}$,
	and $r\le m$ can be an approximate or exact rank of $\valuefunc$. The modes of the reduced basis are the columns of $\bm{\Upsilon}$ which are ideally orthonormal  \ie $\bm{\Upsilon}^\star\, \bm{\Upsilon} = \mathbf{\identity}$. We are concerned with the leading eigen values and eigenvectors of $\valuefunc$; therefore, we project $\valuefunc$ onto the proper orthogonal decomposition (POD) modes in $\bm{\Upsilon}$ according to 
	%
	\begin{align}
		\valuefunc_r = \bm{\Upsilon}^T \valuefunc \bm{\Upsilon}.
	\end{align}
	%
	This reduced model is the Galerkin projection onto the semidiscrete ordinary differential equations (o.d.e.):
	%
	\begin{align}
		\dfrac{d \valuefunc_r}{dt} = \bm{\Upsilon}^T \dfrac{d \valuefunc}{dt} \bm{\Upsilon}.
	\end{align}
	
	For the moment, let us focus on the l.h.s. of \eqref{eq:canonical_value}. Our derivations  closely follow that of Jacobson~\cite{Jacobson1968new}. The major difference is that our choice of $\state_r$ is guaranteed to be close to that of $\state$ so that we need not prescribe stringent conditions for when local control laws are valid on the nonlinear system.  Suppose the optimal terminal cost, $\valuefunc^\star$, is sufficiently smooth to allow a power series expansion in the state variation $\delta \state$ about reduced state, $\state_r$, we find that 
	%
	\begin{align}
		\valuefunc^\star(\state_r + \delta \state, t) &= \valuefunc^\star(\state_r, t) + \left\langle\valuefunc_{\state}, \delta \state \right\rangle + \dfrac{1}{2} \left\langle \delta \state, \valuefunc^\star_{\state\state} \delta\state \right\rangle \nonumber \\
		& \qquad\qquad\qquad  + \text{h.o.t}.
		\label{eq:volterra_expand}
	\end{align}
	%
	Here, h.o.t. signifies higher order terms. This expansion scheme is consistent with  Volterra-series model order reduction methods~\cite{QLMOR} or differential dynamic programming schemes that  decompose nonlinear systems as a summation of Taylor series expansions~\cite{JacobsonMayne}.  Using \eqref{eq:cost_diff},  \eqref{eq:volterra_expand} becomes
	%
	\begin{align}
		\valuefunc^\star(\state_r + \delta \state, t) &= \valuefunc_r(\state_r, t) + \costdiff^\star + \left\langle\valuefunc_{\state}, \delta \state \right\rangle + \nonumber \\
		& \qquad \dfrac{1}{2} \left\langle \delta \state, \valuefunc^\star_{\state\state} \delta\state \right\rangle + \text{h.o.t}.
		\label{eq:Value_expand}
	\end{align}
	%
	The expansion in \eqref{eq:Value_expand} may be more costly than solving for the original value function owing to the large dimensionality of the states as higher order terms are expanded. However, consider:
	%
	\begin{itemize}
		\item $\valuefunc_r(\state_r, t)$ already contains the dominant modes of $\valuefunc(\state, t)$ as a result of the singular value decomposition scheme; therefore w.l.o.g. states in the reduced order basis (ROB), $\valuefunc_r(\state_r, t)$, will be sufficiently close to those that originate in \eqref{eq:sys_dyn};
		%
		\item If the above is true, the state variation  $\delta \state$ will be sufficiently small owing to the fact that $\state \approx \state_r$  \cf \eqref{eq:variations}.
		%
	\end{itemize}
	%
	\item Therefore, we can avoid the infinite data storage requirement by truncating the expansion in \eqref{eq:Value_expand} at, say, the quadratic (second-order) terms in $\delta \state$. Seeing that $\delta \state$ is sufficiently small, the second-order cost terms will dominate higher order terms, and this new cost will result in an $O(\delta \state^3)$ approximation error, affording us realizable control laws that can be executed on the system \eqref{eq:sys_dyn}. From \eqref{eq:volterra_expand}, we have
	%
	\begin{align}
		\valuefunc^\star(\state_r + \delta \state, t) &= \valuefunc_r + \costdiff^\star + \left\langle\valuefunc_{\state}, \delta \state \right\rangle +  \dfrac{1}{2} \left\langle \delta \state, \valuefunc^\star_{\state\state} \delta\state \right\rangle.
		\label{eq:val_rom_expand}
	\end{align}
	%
	Denoting by $\valuefunc_{\state}^\star$ the co-state on the r.h.s of \eqref{eq:canonical_value}, we  can similarly expand it up to second order terms as follows
	%
	\begin{align}
		\valuefunc_{\state}^\star\left(\state_r + \delta \state, t \right) = \dfrac{\partial \valuefunc^\star_r}{\partial \state}\left(\state_r , t \right) + \langle \valuefunc^\star_{\state\state}\left(\state_r , t \right), \delta \state \rangle.
		\label{eq:co_state_expand}
	\end{align}
	%
	Note that \textit{the co-state in \eqref{eq:co_state_expand} and parameters on the r.h.s. of \eqref{eq:val_rom_expand} are evaluated on the reduced model, specifically at the phase $\left(\state_r, t\right)$}. Substituting \eqref{eq:val_rom_expand} and \eqref{eq:co_state_expand} into \eqref{eq:canonical_value}, abusing notation by dropping the superscripts and the templated phase arguments, we find that
	%
	\begin{align}
		\begin{split} 
			-\frac{\partial \valuefunc_r}{\partial t} -\frac{\partial \costdiff}{\partial t} - \left\langle \dfrac{\partial\valuefunc_{\state}}{\partial t}, \delta\state \right \rangle -  \dfrac{1}{2} \left\langle \delta \state, \dfrac{\partial \valuefunc_{\state\state} }{\partial t}\delta\state \right \rangle &=  \\
			\min \left\{0,  
			\max_{\delta \control} \, \min_{\delta \disturb} \left\langle f^T(t; \state_r + \delta \state, \control_r + \delta \control,  \disturb_r + \delta \disturb), \right. \right. \\
			\left. \left. \valuefunc_{\state} +  \valuefunc_{\state\state}\, \delta \state \right\rangle \right\}. 
		\end{split}
		\label{eq:ROM_HJI_Proof}
	\end{align}
	%
	%\textit{A fortiori}, we have the optimal Galerkin approximation of the HJI variational problem   \cf \eqref{eq:lower_hji_pde} with the ROM \eqref{eq:ROM_HJI_Proof}. 	
	Observe that $\valuefunc_r+ \costdiff$, $\valuefunc_{\state}$, and $\valuefunc_{\state\state}$ are all functions of the phase $\left(\state, r\right)$ so that 
	\begin{subequations}
		\begin{align}
			\frac{d}{dt}\left(\valuefunc_r + \costdiff\right) &= \dfrac{\partial}{\partial t}\left(\valuefunc_r + \costdiff\right) + \left\langle f^T(t; \state_r, \control_r, \disturb_r), \valuefunc_{\state} \right \rangle \\
			%
			\dot{\valuefunc}_{\state} &= \dfrac{\partial \valuefunc_{\state\state}}{\partial t} + \langle f^T(t; \state_r, \control_r, \disturb_r),  \valuefunc_{\state\state} \rangle \\
			%
			\dot{\valuefunc}_{\state\state} &= \dfrac{\partial \valuefunc_{\state\state}}{\partial t}. %+ \langle f^T(t; \state_r, \control_r, \disturb_r), \valuefunc_{\state\state} \rangle
		\end{align}
		\label{eq:reduced_canonical}
	\end{subequations}
\end{proof} 
%
The left hand side of \eqref{eq:ROM_HJI_Proof} admits a quadratic form, so that we can regress a quadratic form to fit the functionals and derivatives of the optimal structure of the ROB. The r.h.s. can be similarly expanded as above.   Define 
%
\begin{align}
	\hamfunc(t; \state, \control, \disturb, \valuefunc_{\state}) = \langle \valuefunc_{\state}, f(t; \state, \control, \disturb) \rangle 
\end{align}
%
so that \eqref{eq:ROM_HJI_Proof} becomes
%
\begin{align}
	\begin{split} 
		-\frac{\partial \valuefunc_r}{\partial t} -\frac{\partial \costdiff}{\partial t} - \left\langle \dfrac{\partial\valuefunc_{\state}}{\partial t}, \delta\state \right \rangle -  \dfrac{1}{2} \left\langle \delta \state, \dfrac{\partial \valuefunc_{\state\state} }{\partial t}\delta\state \right \rangle &=  \\
		\min \left\{\bm{0},  
		\max_{\delta \control} \, \min_{\delta \disturb} \left[\hamfunc(t; \state_r + \delta \state, \control_r + \delta \control, \disturb + \delta \disturb, \valuefunc_{\state}) + \right. \right. \\
		\left. \left.  \left\langle \valuefunc_{\state\state}\, \delta \state, f(t; \state_r + \delta \state, \control_r + \delta \control,  \disturb_r + \delta \disturb)  \right\rangle\right] \right\}. 
	\end{split}
\label{eq:rob_expand}
\end{align}

Expanding the r.h.s. about $\state_r, \control_r, \disturb_r$ up to second-order only\footnote{This is because the \lhs was truncated at  second order expansion previously. Ultimately, the $\delta \control, \delta \disturb$ terms will be quadratic in $\delta \state$ if we neglect h.o.t.}, we find that
%
\begin{align}
	\begin{split} 
		\min \left\{\bm{0},  
		\max_{\delta \control} \, \min_{\delta \disturb} \left[\hamfunc + \left\langle \hamfunc_{\state}  + \valuefunc_{\state \state} f, \delta \state\right\rangle + \right. \right. \\
		\left. \left. \langle \hamfunc_{\control }, \delta \control \rangle + \langle \hamfunc_{\disturb }, \delta \disturb \rangle +  \langle \delta \control, (\hamfunc_{\control  \state} + f_{\control}^T \valuefunc_{\state \state}) \delta \state  \rangle \right. \right. \\
		\left. \left. 
		%
		+  \left\langle \delta \disturb, (\hamfunc_{\disturb  \state} + f_{\disturb}^T \valuefunc_{\state \state}) \delta \state  \right\rangle + \dfrac{1}{2}\left\langle\delta \control, \hamfunc_{\control  \disturb} \delta \disturb  \right\rangle  \right. \right. \\
		\left. \left. 
		%
		 + \dfrac{1}{2}\left\langle\delta \disturb, \hamfunc_{\disturb  \control} \delta \control \right\rangle +  \dfrac{1}{2} \left\langle \delta \control, \hamfunc_{\control  \control} \delta \control \right\rangle +  \dfrac{1}{2} \left\langle \delta \disturb, \hamfunc_{\disturb  \disturb} \delta \disturb \right\rangle \right. \right. \\
		\left. \left. 
		%
		+    \dfrac{1}{2} \left\langle \delta \state, \left(\hamfunc_{\state \state} + f_{\state}^T \valuefunc_{\state \state}  + \valuefunc_{\state \state} f_{\state} \right) \delta \state \right\rangle \right] \right\}. 
	\end{split}
\label{eq:rob_rhs}
\end{align}
%Suppose that the extrema controls are $\control^\star = \control_r + \delta \control^\star   

Let us recall that %the policy pair $\{\control(t), \disturb(t)\}$ constitute a saddle-point solution  to the differential game %\eqref{eq:sys_dyn}, \eqref{eq:payoff}, 
%with final time $t_f = \inf \{t \in \bb{R}^+: (\state(t), t) \in \mathcal{L}_0\}$ if 
%%
%\begin{align}
%	J(\control^\star(t), \disturb(t)) \le J(\control^\star(t), \disturb^\star(t)) \le J(\control(t), \disturb^\star(t)).
%	\label{eq:saddle_points}
%\end{align}
%
when capture\footnote{A capture occurs when $\evader$'s separation from $\pursuer$ becomes less than a specified \eg capture radius.} occurs, we must have the Hamiltonian of the value function be zero as a necessary condition for the players' saddle-point controls~\cite{Merz1972,Isaacs1965} \ie
%
%\begin{align}
%	\hamfunc_u(t; \state, \bm{u}^\star, \disturb, p) =0, \,\hamfunc_v(t; \state, \bm{u}, \disturb^\star, p) =0,
%\end{align} 
%   
%We have that $\control^\star$ maximizes $\valuefunc$ and $\disturb^\star$ minimizes $\valuefunc$ so that the saddle solution's necessary conditions imply              
%
\begin{align}
	\hamfunc_{\control}(t; \state_r, \control_r^\star, \disturb_r, \valuefunc_{\state}) = 0; \,\hamfunc_{\disturb}(t; \state_r, \control_r, \disturb_r^\star, \valuefunc_{\state}) = 0.
	\label{eq:saddle_funcs}
\end{align}                                                                       %
where $\control_r^\star$ and $\disturb_r^\star$ respectively represent the optimal control laws for both players at time $t$.

A state-control relationship of the following form is sought:
%
\begin{align}
\delta \control = \gain_{\control} \delta \state, \quad \delta \disturb = \gain_{\disturb} \delta \state
\label{eq:gains}
\end{align}
%
so that \eqref{eq:rob_rhs} in the context of  \eqref{eq:saddle_funcs}  yields 
%
\begin{subequations}
	\begin{align}
		\hamfunc_{\control} &+ \hamfunc_{\control  \control} \delta \control  + \left(\hamfunc_{\control  \state} + f_{\control}^T \valuefunc_{\state \state}\right) \delta \state + \dfrac{1}{2}\hamfunc_{\control  \disturb} \delta \disturb = 0 \\
		%
		\hamfunc_{\disturb} &+ \hamfunc_{\disturb  \disturb} \delta \disturb  + \left(\hamfunc_{\disturb  \state} + f_{\disturb}^T \valuefunc_{\state \state}\right) \delta \state + \dfrac{1}{2}\hamfunc_{\disturb\control} \delta \control = 0.
	\end{align}
\end{subequations}
%
Using \eqref{eq:saddle_funcs} and equating like terms in the resulting equation to  those in \eqref{eq:gains}, we have the following for the state gains:
%
\begin{align}
	\gain_{\control} &= - \dfrac{1}{2}\hamfunc_{\control  \control}^{-1} \left[ \hamfunc_{\control  \disturb}\gain_{\disturb}  + 2\left(\hamfunc_{\control  \state} + f_{\control}^T \valuefunc_{\state \state}\right) \right], \text{ and} \\
	%
	\gain_{\disturb} &= - \dfrac{1}{2}\hamfunc_{\disturb  \disturb}^{-1} \left[\hamfunc_{\disturb  \control} \gain_{\control} +  2\left(\hamfunc_{\disturb  \state} + f_{\disturb}^T \valuefunc_{\state \state}\right)\right]. \nonumber
\end{align}        

Putting the maximizing $\delta \control$ and the minimizing $\delta \disturb$ into \eqref{eq:rob_rhs}, whilst neglecting terms in $\delta \state$ beyond second-order, we have
%
\begin{align}
	\begin{split} 
		\min \left\{\bm{0},  
		 \left[\hamfunc + \left\langle \left(\hamfunc_{\state}  + \valuefunc_{\state \state} f + \gain_{\control}^T \hamfunc_{\control} + \gain_{\disturb}^T \hamfunc_{\disturb}\right), \delta \state \right\rangle  \right. \right. \\ \left. \left.
		 %
		+    \dfrac{1}{2} \left\langle \delta \state, \left(\hamfunc_{\state \state} + f_{\state}^T \valuefunc_{\state \state}  + \valuefunc_{\state \state} f_{\state}  + \gain_{\control}^T \hamfunc_{\control  \control}  \gain_{\control} 
		 \right.\right. \right. \right.  \\ \left. \left. \left. \left. 
		%
		+ \gain_{\disturb}^T \hamfunc_{\disturb  \disturb}  \gain_{\disturb} \right) \delta \state \right\rangle \right] \right\}. 
	\end{split}
	\label{eq:rob_ham_analytical}
\end{align}
%
Now, we can compare coefficients with the \lhs of \eqref{eq:rob_expand} and find the quadratic expansion of the reduced value function admits the following analytical solution on its right hand side:
%
\begin{subequations}
	\begin{align}
		%\begin{split}
		-\dfrac{\partial \valuefunc_r}{\partial t} - \dfrac{\partial \tilde{\valuefunc}}{\partial t} = \min\{\bm{0}, \hamfunc\} \\
		%
		-\dfrac{\partial \valuefunc_{\state}}{\partial t} = \min \left\{\bm{0}, \hamfunc_{\state} + \valuefunc_{\state\state} \, f + \gain_{\control}^T \hamfunc_{\control } + \gain_{\disturb}^T \hamfunc_{\disturb} \right\} \\
		%
		-  \dfrac{\partial \valuefunc_{\state \state}}{\partial t} = \min \left\{\bm{0}, \hamfunc_{\state \state} + f_{\state}^T\valuefunc_{\state\state} + \valuefunc_{\state\state} f_{\state}  \right. \nonumber \\
		\left.
		+ \gain_{\control}^T \hamfunc_{\control  \control}  \gain_{\control} +  \gain_{\disturb}^T \hamfunc_{\disturb  \disturb}  \gain_{\disturb} \right\}.
		%\end{split}
		%		
	\end{align}
\end{subequations}
%
%
Furthermore, comparing the above with \eqref{eq:reduced_canonical} and noting that $-\dot{\valuefunc}_r = 0$\footnote{The stage cost is zero from \eqref{eq:payoff}.}, we find that 
%
\begin{subequations}
	\begin{align}
		%\begin{split}
		- \dot{ \tilde{\valuefunc}} = - \dfrac{\partial \tilde{\valuefunc}}{\partial t} \triangleq \min\{\bm{0}, \hamfunc - \hamfunc(t; \state_r, \control_r, \disturb_r, \valuefunc_{\state})\} \\
		%
		-\dot{\valuefunc}_{\state} = \min \left\{\bm{0}, \hamfunc_{\state} + \valuefunc_{\state\state} \left(f-f(t; \state_r, \control_r, \disturb_r)\right)  \right. \\
		\left.
		+ \gain_{\control}^T \hamfunc_{\control } + \gain_{\disturb}^T \hamfunc_{\disturb} \right\} \\
		%
		-  \dfrac{\partial \valuefunc_{\state \state}}{\partial t} = \min \left\{\bm{0}, \hamfunc_{\state \state} + f_{\state}^T\valuefunc_{\state\state} + \valuefunc_{\state\state} f_{\state}  \right. \nonumber \\
		\left.
		+ \gain_{\control}^T \hamfunc_{\control  \control}  \gain_{\control} +  \gain_{\disturb}^T \hamfunc_{\disturb  \disturb}  \gain_{\disturb} \right\}
		%\end{split}
		%		
	\end{align}
\end{subequations}
%
where $\gain_{\control}$ and $\gain_{\disturb}$ are as defined in \eqref{eq:gains}. Note that at a saddle point, the first-order necessary condition for optimality \cf \eqref{eq:saddle_funcs} implies  %$\hamfunc_{\control}(t; \state_r, \control^\star, \disturb^\star) = 0$ and $\hamfunc_{\disturb}(t; \state_r, \control^\star, \disturb^\star) = 0$ so that
%
\begin{subequations}
	\begin{align}
		- \dot{ \tilde{\valuefunc}} = \min\{0, \hamfunc - \hamfunc(t; \state_r, \control_r, \disturb_r, \valuefunc_{\state})\} \\
		%
		-\dot{\valuefunc}_{\state} = \min \left\{0, \hamfunc_{\state} + \valuefunc_{\state\state} \left(f-f(\state_r, \control_r, \disturb_r)\right) \right\} \\
		%
		-  \dfrac{\partial \valuefunc_{\state \state}}{\partial t} = \min \left\{\bm{0}, \hamfunc_{\state \state} + f_{\state}^T\valuefunc_{\state\state} + \valuefunc_{\state\state} f_{\state}  \right. \nonumber \\
		\left.
		+ \gain_{\control}^T \hamfunc_{\control  \control}  \gain_{\control} +  \gain_{\disturb}^T \hamfunc_{\disturb  \disturb}  \gain_{\disturb} \right\}
	\end{align}
	\label{eq:saddle_reduce}
\end{subequations}
%
whereupon every quantity in \eqref{eq:saddle_reduce} is evaluated at $\state_r, \control^\star$.

The boundary conditions for \eqref{eq:saddle_reduce} at $t = 0$ is 
%
\begin{align}
	\valuefunc(\state_r, 0) = \valueterm(0; \state_r(0));
\end{align} 
%
so that
%
\begin{subequations}
	\begin{align}
		\tilde{\valuefunc}(0) &= 0 \\
		\valuefunc_{\state}(0) &= \valueterm_{\state} (0; \state_r(0)) \\
		\valuefunc_{\state \state} (0) &= \valueterm_{\state \state} (0; \state_r(0)).
	\end{align}
\end{subequations}

The following control laws are then applied 
%
\begin{align}
	\control &= \control_r +  \gain_{\control} \delta \state, \\
	%
	\disturb &= \disturb_r +  \gain_{\disturb} \delta \state.	
\end{align}

Therefore, at any time on a ROB of the value function, a local approximation of $\valuefunc$ consists in employing the \todo{Lax-Friedrichs scheme} on the following system
%
\begin{align}
	\begin{split}
		-\left[\bm{E} + \bm{F} \delta \state + \dfrac{1}{2} \delta \state \bm{G} \delta \state  \right] = \min \left\{\bm{0}, \hamfunc    \right. \\ \left. % \left.
		- \hamfunc(t; \state_r, \control_r, \disturb_r, \valuefunc_{\state})+ \hamfunc_{\state}+ \valuefunc_{\state\state} \left(f-f(t; \state_r, \control_r, \disturb_r)\right) \right. \\ \left. 
		 + \hamfunc_{\state \state} 
		+ f_{\state}^T\valuefunc_{\state\state} + \valuefunc_{\state\state} f_{\state} 	+ \gain_{\control}^T \hamfunc_{\control  \control}  \gain_{\control} +  \gain_{\disturb}^T \hamfunc_{\disturb  \disturb}  \gain_{\disturb} \right\}
	\end{split}
\end{align}
%
where $ \bm{E}, \bm{F},  \, \text{ and } \,\bm{G}$ are appropriately defined.

\begin{comment}
%
%By definition, backward reachable tubes (BRTs) are the sublevel sets of the viscosity solutions to 
The numerical resolution of hyperbolic\footnote{Hyperbolic PDEs are those that depend on at most the first-order partial derivatives of the vector field under consideration.} HJ PDEs~\cite{Mitchell2020} in a viscosity sense comes from the resolution of scalar continuous conservation laws e.g. the conservation of electric charge density, $\rho$ (in $\text{Coulombs}/m^{2}$), of an electromagnetic field  i.e.,
%
\begin{align}
\dfrac{\partial \rho}{\partial t} +(\nabla \cdot \bm{J}) = 0
\label{eq:conserve}
\end{align}
%
where $\bm{J}$ is the current density (in $Amperes/m^{2}$). If we relax the viscous properties of these conservation laws, they are effectively reduced to compressible inviscid  Euler equations with discontinuities~\cite{LevelSetsBook}. The discontinuities in the Euler dynamics of such inviscid conservation equations make their  resolution extendable to the numerical solution of HJI PDEs. We restate the following culminations from \cite{OsherShuENO} to build the case for our decomposition scheme.
%
\begin{remark}
The solution (\eg $\rho$ from \eqref{eq:conserve}) to a conservation law is tantamount to the derivative of the solution to an HJ PDE \textbf{along a single spatial dimension}.
\end{remark}
%
%
\begin{remark}
The solution of an HJ PDE, e.g. $\bm{u}$ or $\bm{v}$ in \eqref{eq:lower_visc}, \textbf{along a single spatial dimension}, is the integral of the solution of a scalar conservative equation, \eg $\rho$ in \eqref{eq:conserve}.
\label{rem:integral_conserve}
\end{remark}
%
%The above two remarks are conclusions drawn from \cite{OsherShuENO}. 
Remark \autoref{rem:integral_conserve} informs us that HJ solutions obtained by integrating conservation law PDEs that possess discontinuous dynamics may lead to kinks in the HJ solutions\footnote{Since the integral of a discontinuity is a kink.}. For our proposed decomposition of BRS/BRT, if a  decomposed HJ PDE's solution (to be introduced shortly) develops kinks in its solution, a form of bang-bang control needs be considered if indeed the partial derivatives are discontinuous in those regions of the state space. If the conservation law does not develop a delta function, we can discount accounting for these discontinuities as the HJ solution will be continuous.  In the level set methods, to assure a monotone decrement in the value function, Lax-Friedrich schemes with carefully chosen artificial dissipation coefficients are used in computing solutions to the discretized Hamiltonian in order to account for constant or varying spatial dynamics\footnote{Although in the level sets toolbox, dissipation coefficients are chosen with the assumption that the underlying dynamics are spatially constant; this is very limiting for environments with unpredictable dynamics.} and alleviate possible numerical inconsistencies in computed gradients to the HJ PDE\footnote{This is discussed at length in ~\cite[\S5.3.1]{LevelSetsBook}.}.
\end{comment} 

\subsection{Decomposition Rationale.}
Multilinear compositions of linear forms are an efficient way of manipulating complex systems.   Higher-order tensors, in particular, are increasingly playing crucial roles in the storage, analysis, and use of high-dimensional data. Applications range from deep learning, higher-order statistics, chemometrics, psychometrics to signal processing inter alia. Evidence abounds that linearized nonlinear system dynamics, truncated at a reduced order $r$-th mode (e.g. in power series expansions\cite{iDG, JacobsonMayne, Mitter1966, McReynolds1967}) admit a higher precision and accuracy of the approximation of the underlying nonlinear system since the moments and accumulations of higher-order dynamics are equivalent to the power series expansion coefficients.  



In this section, we introduce a multilinear decomposition scheme for decomposing large backward reachable tubes in order to alleviate the exponential complexity of mesh constraints; it is an iterative scheme that generates separable reduced order models (ROM) of the original value function, which are respectively compactly represented on a mesh -- making our method amenable to resolving terminal value functions using level sets methods. 
In~\cite{Mitchell2005}, Mitchell showed that by giving the pursuer an advantage with a  nonanticipative strategy in the two-person game, the reachable set is overapproximated in the Lax-Friedrichs numerical scheme used. In our decomposition scheme, we follow the same non-anticipative strategies for the pursuer on the reduced basis. Hence, our decomposition scheme admits an overapproximation of the reachable set in some sense.



\subsection{Separable Representations of the ROM Cost Functional}
% 
Let $\mc{S}$ be a measurable multidimensional set, including one of possibly infinite dimensions such as $\{\hilbertcoeff_i\}_{i=0}^\infty \in \mc{S}$. Furthermore, let  the functional space $L^2(\mc{S}; \mc{F})$ be the class of functions $\{\hilbertparam_i(\state)\}_i^\infty \in \mc{F}$ whose second powers are measurable over the set $\mc{S}$ and for which 
%
\begin{align}
	%L^2(\mc{S}; \mc{F}) = \left\{\hilbertparam \, \bigg| \,\hilbertparam \in  \mc{F},
	\int_{\mc{S}} \hilbertcoeff(t)  \, \| \hilbertparam(\state)\|^2 \,\, d\state < +\infty.
	% \right\}.
\end{align}
%
Suppose that for another $\hilbertparam^\prime(\state) \in \mc{F}$, we have
%
\begin{align}
	\langle \hilbertparam(\state), \hilbertparam^\prime(\state) \rangle = \int_{\mc{S}} \psi(t) \, \left[\hilbertparam(\state) \, \overline{\hilbertparam^\prime(\state)} \right] \, dt,
\end{align}
%
then $L^2(\mc{S}; \mc{F})$ becomes a Hilbert space, where $\bar{\hilbertparam^\prime}(\state)$ is the complex conjugate of $\hilbertparam(\state)$. The finiteness of $\langle \hilbertparam(\state), \hilbertparam^\prime(\state) \rangle$ follows from Bunyakovskii's inequality \ie $|\langle \hilbertparam(\state), \hilbertparam^\prime(\state) \rangle| \le \infty$.
In addition, the inner product associated with  $L^2(\mc{S}, \mc{F})$ induces a norm\footnote{This norm will be denoted by $\| \cdot \|_{{F}}$. When we are abusing notation, we will simply write this norm or its inner product without the  $F$ subscript.}  in $L^2(\mc{S}; \mc{F})$ which is given by %on the tensor product space $\mathcal{F} \otimes \mathcal{S}$ as follows 
%
%\begin{align}
%	\langle &\hilbertcoeff, \hilbertparam \rangle = \int_{\mathcal{S}} \langle \hilbertcoeff, \hilbertparam \rangle d\state.
%	\label{eq:frob_inner_prod}
%\end{align}  
%
\begin{align}
	\| \hilbertparam(\state) \| = \left[\int_{\mc{S}} \hilbertcoeff(t) \| \hilbertparam(\state) \|^2 d\state \right]^{\frac{1}{2}}.
\end{align}
%
The space $L^2(\mc{S}; \mc{F})$ is separable so that we can take a denumerable dense set of polynomial expansions in it~\cite{Kantarovich}.%, which may for example correspond to power series expansions in $\hilbertparam(\state)$ about $\state$ with rational coefficients $\hilbertcoeff(t)$, truncated at an order, $n$.  

\subsection{Decomposition Layout}
%
\noindent %\textbf{Problem Statement}: 
 Following the outline of the denumerable construction of functions $\hilbertparam(\state) \in \lpspace$ above, we consider separated representations of the value function $\valuefunc (\state,t)$  into time functions alone \ie $\hilbertcoeff(t) \in \mc{S}$ and space functions alone \ie $\hilbertparam(\state) \in \mathcal{F}$. Let us call this approximation  $\valuefunc (\state,t; \hilbertcoeff)$, and let it possess real values on  $L^2(\mc{S}; \mc{F})$. % so that it satisfies the  boundary condition \eqref{eq:lower_visc_boundary}. 

In this sentiment, $\valuefunc (\state,t; \hilbertcoeff)$ is the sum of the tensor products of rational coefficients $\{\hilbertcoeff_i\}_{i=0}^\infty \in \mathcal{S}$ and polynomial basis functions $\{\hilbertparam_i\}_{i=0}^\infty \in \mathcal{F}$ that satisfy
%
\begin{subequations}
	\begin{align}
		\valuefunc(\state, t) &\approx \valuefunc(\state, t; \hilbertcoeff) \\
		&\equiv \sum_{i=0}^\infty  \hilbertcoeff_i(t) \, \hilbertparam_i(\bm{x}), \quad \hilbertcoeff_i \in  \mathcal{S}, \, \hilbertparam_i \in \mathcal{F}.
	\end{align} 
	\label{eq:decomp_general}
\end{subequations}
%

A parameterized P.D.E that admits a separable representation $\valuefunc(\state, t; \hilbertcoeff)$ of order $r$ can be defined as the function
%
\begin{align}
	\valuefunc_r(\state, t) = \sum_{i=0}^{r-1}  \hilbertcoeff_i(t) \, \hilbertparam_i(\bm{x}), \quad \hilbertparam_i \in   \mathcal{S}, \, \hilbertparam_i \in \mathcal{F}.
	\label{eq:decomp_reduced}
\end{align} 
% 
These \textit{summands over tensor products constitute the Galerkin decomposition of the viscosity solution} $\valuefunc (\state,t)$. %If $r<N$, we have snapshots, otherwise, we have principal components. 
We successively solve for the most energetic and orthogonal modes $\hilbertparam_k(\state)$, $k=1, \cdots, n$, established  via the Galerkin orthogonality criteria 
%
\begin{align}
	\| \valuefunc - \valuefunc_r \|^2_{F} = \min_{\substack{\{\hilbertcoeff_i\}_{i=1}^\infty \in \mathcal{S} \\ \{\hilbertparam_i\}_{i=1}^\infty \in \mathcal{F} }} \|\valuefunc - \sum_{i=0}^{r-1}  \hilbertcoeff_i(t)\, \hilbertparam_i(\state) \|_F^2.
	\label{eq:galerk_orth_crit_reduced}
\end{align}

\noindent   Suppose that a reduced-order basis (ROB) is already known. In an iterative fashion, we construct a reduced-order model (ROM) that solves \eqref{eq:decomp_reduced} and examine the sufficiency of the solution w.r.t \eqref{eq:lower_hji_pde}. \todo{In the advent of an  insufficient solution, the current ROM is \textit{enriched} with new proper generalized decomposition (PGD) functions. %These PGD functions are obtained  %using a greedy power algorithm by minimizing a functional which is generated from a verification of the search direction. 
We adopt this PGD scheme since it has been shown to reduce the computational and storage cost of similar problems in multiscale analysis and Navier-Stokes equations~\cite{Nouy2009,LadevezeBook,ChinestaPGD,NouyFEM,Nouy2010}}.

We now resolve the problem stated in the foregoing. Consider an environment where $\valuefunc \in \mathbb{R}^{I_0 \times I_1 \times \cdots \times I_{N-1}}$ is a high-dimensional value function for a system  of multiple interacting agents, each with dynamics $\dot{\state_0}, \, \dot{\state_1}, \, \cdots \dot{\state_n}$, and whose respective state spaces span the full rank of each mode of $\valuefunc$. Since the value functions for such a system are of high order, we replace the value function $\valuefunc(\state, t)$ with its tensor representation,  $\mathds{V}(\state, t)$ (\cf \autoref{sec:notations}) so that  the full problem described in Lemma \ref{lemma:lower_visc_lemma}, corresponds to the following parameterized P.D.E 
%
\begin{subequations}
	\begin{align}
		 -\dfrac{\partial \valuetensor}{\partial t}(\state, t; \hilbertcoeff) &= \wtensor(\valuetensor; \hilbertcoeff), %\hamtensor (\state, t, p; \hilbertparam) = 0,
		  \, t \in \left[T, 0\right],  x \in \openset, \hilbertcoeff \in \mc{S}  \\
		%
		\valuetensor(\state, T; \hilbertcoeff) &= g(\state; \hilbertcoeff) \quad \state \in  \{ \bar{\openset} \backslash \text{int } \openset\}, \,  \hilbertcoeff \in \mc{S}
		\label{eq:lower_visc_boundary_param}
	\end{align}
	\label{eq:lower_visc_param}
\end{subequations}
% 
whereupon $\valuetensor(\state, T; \hilbertcoeff)$ is contained in $\lpspace$ and $\wtensor(\valuetensor; \hilbertcoeff)$ is contained in  the dual space $\lpdual$ that is associated with the space $\lpspace$. 

\textbf{The decomposition problem is to find}
%
$\valuetensor \in L^2(\mc{S}; \mc{F})$ such that $\dfrac{\partial \valuetensor}{\partial t} \in L^2(\mc{S}; \breve{\mc{F}})$ satisfies the boundary conditions to the initial value problem \eqref{eq:lower_hji_pde} and 
%
\begin{align}
	\left\langle \hilbertparam, \dfrac{\partial \valuetensor}{\partial t} \right\rangle = \left\langle \hilbertparam, \wtensor(\valuetensor; \hilbertcoeff) \right\rangle \quad \forall \, \hilbertparam \in \mc{F}, \, \hilbertcoeff \in \mc{S}
	\label{eq:prob_statement}
\end{align}
%
in a weak sense. The decomposition of \eqref{eq:decomp_reduced} can be considered a pseudo-eigenvalue problem which proves efficient for separated representations in many applications including stochastic nonlinear PDEs~\cite{Nouy2009, LadevezeBook} and finite element methods~\cite{NouyFEM}. The state space can be split into disjoint regions where the value function is continuously differentiable in each region. \todo{The \textit{singular surfaces}~\cite{Isaacs1965} that separate the respective disjoint value functions constitute manifolds which have discontinuous derivative properties and we follow ~\cite[Theorem 8.2]{BasarBook}'s manifold resolution strategy \ie these manifolds satisfy the saddle equilibrium strategies}
%
\begin{align}
	\hamtensor(t; \state, \control^\star, \disturb) \le \hamtensor(t; \state, \control^\star, \disturb^\star) \le \hamtensor(t; \state, \control, \disturb^\star). 
	\label{eq:saddle_ham}
\end{align}
%
\todo{When the value function is not continuously differentiable; or the value function becomes discontinuous, we resort to  classical fractional steps in finite differencing schemes for conservation laws~\cite{CrandallFractional} applied on a dimension-by-dimension basis to a mesh on which a separated composition is defined~\cite{OsherShuENO}}.
We defer the treatment of \textit{dispersal surfaces}~\cite{Isaacs1965} to a future work.

\subsection{Galerkin approximation of the Variational HJI Problem}
\label{subsec:galerkin}
%
We now derive the Galerkin approximation of the viscosity solution to the terminal HJI problem. Assume that a decomposition $V_{r}$ of order $r$ is already known (this could be obtained by a partial truncation of the value function as described in \autoref{subsec:inc_hosvd}) or randomly initialized. For the next order $r+1$, a new couple $\left(\hilbertcoeff, \hilbertparam\right)$ is optimal if it satisfies the Galerkin orthogonal metric %on the induced norm of the tensor product space $\mathcal{S}\otimes \mathcal{F}$ such that 
%
\begin{align}
	\| \mathds{V} - \mathds{V}_{r} \|_F^2 &= \|\mathds{V} \|_F^2 - \sigma(\phi_i(\state)) \nonumber \\
	&\equiv \|\mathds{V} \|_F^2 - \| \valuecore \|_F^2,
	\label{eq:galerk_orth_crit}
\end{align}
%
where $\sigma(\hilbertparam_i(\state))$ denotes an eigen decomposition of $\hilbertparam_i(\state)$ and $\valuecore$ is the \textit{core tensor} of $\valuetensor$ -- representing its critical mass -- which can be obtained e.g. from a high order orthogonal iteration~\cite{Kolda2009} or from a higher order singular value decomposition~\cite{DeLathauwer2000, VannieuwenhovenTruncate2012} of $\valuefunc$ (We discuss this in \autoref{subsec:inc_hosvd}). The optimality proof of \eqref{eq:galerk_orth_crit} is given in appendix \ref{app:ortho_proj_error} via \eqref{eq:core_tensor}. 	

\todo{\begin{theorem}
	Let  the set $\{\hilbertcoeff\}_{i=0}^{r-1} \in \mc{S}$ and functions $\{\hilbertparam\}_{i=0}^{r-1} \in \mc{F}$ be vectorized as 
	%
	\begin{align}
		\hilbertcoeffspace_r^T &= \left[\hilbertcoeff_0, \cdots, \hilbertcoeff_{r-1}\right], \,\,
		%
		\hilbertparamspace_r = \left[\hilbertparam_0, \cdots, \hilbertparam_{r-1}\right]^T,
		\label{eq:galerk_vectorize}
	\end{align}
%Furthermore, let $\hilbertcoeffspace_r^\star$, $\hilbertparamspace_r^\star$ denote the optimal sets and basis modes, 
then it follows that if the quasi-optimal basis modes have been found \footnote{The rationale for introducing quasi-optimality is that the best rank decomposition of the value function's tensor representation to a low-order rank tensor approximation admits accuracy only to a factor of the square root of the number of modes of the best approximation~\cite[Theorem 6.27]{Hackbusch} \ie
	%
	\begin{align}
		\| \valuetensor - \hat{\valuetensor} \| \le \sqrt{N} \| \valuetensor - \hat{\valuetensor}_{opt} \| \le O(n^{3/2})
	\end{align} 
	%
	where $\valuetensor_{opt}$ is the best rank-$R_0 \times R_1 \times \cdots \times R_{N-1}$ approximation of $\valuetensor$ and $\hat{\valuetensor}$ can be obtained from either Algorithm \ref{alg:tucker_power_iter} or \ref{alg:st-hosvd}.
}, the optimal Galerkin approximation of the HJI variational problem is given by
%
\begin{align}
%-\hilbertcoeffspace_r^T \, \dot{\hilbertparamspace}_r 
 \dfrac{\partial \valuetensor}{\partial t}\approx  \dfrac{\partial \valuetensor_r}{\partial t} = \ \min \left\{0, \max_{\control \in \mathcal{U}} \, \min_{\disturb \in \mathcal{V}} \left\langle f(t; \state, \control, \disturb),  \hilbertcoeffspace_r^T \, \hilbertparamspace_{\state}\right\rangle \right\}. %\nonumber \\
%
% &= \min \left\{0, \Psi_r^T \cdot \left[\max_{\control \in \mathcal{U}} \, \min_{\disturb \in \mathcal{V}} \left\langle f(t; \state, \control, \disturb), \Phi_{\state} \right\rangle_F \right] \right\}.
\end{align}
\todo{Work with matrices here and introduce the higher order decomposition after section \ref{subsec:inc_hosvd}}.
\end{theorem}
}

	The optimal Galerkin-approximation of the r.h.s of \eqref{eq:lower_visc_param} gives 
%
\begin{align}
	\begin{split}
		-\frac{\partial \valuetensor_r}{\partial t}(\state_r, t) &= 
		\min \left\{0,  
		\max_{\control \in \mathcal{U}} \, \min_{\disturb \in \mathcal{V}} \left\langle f(t; x, \control, \disturb), \right. \right. \\
		& \qquad \qquad \quad \left. \left. \left(\sum_{i=0}^{r-1} \psi_i \, \dfrac{\partial }{\partial \state}\phi_i \right) \right\rangle \right\} 
	\end{split}
	\label{eq:galerk_decomp}
\end{align}
%
The left hand side of \eqref{eq:galerk_decomp} 

%
So that given \eqref{eq:galerk_vectorize}, we can rewrite \eqref{eq:galerk_decomp} as 
%
\begin{align}
	-\hilbertcoeffspace_r^T \, \dot{\hilbertparamspace}_r  &= \min \left\{0, \max_{\control \in \mathcal{U}} \, \min_{\disturb \in \mathcal{V}} \left\langle f(t; \state, \control, \disturb),  \hilbertcoeffspace_r^T \, \hilbertparamspace_{\state}\right\rangle \right\}. %\nonumber \\
	%
	% &= \min \left\{0, \Psi_r^T \cdot \left[\max_{\control \in \mathcal{U}} \, \min_{\disturb \in \mathcal{V}} \left\langle f(t; \state, \control, \disturb), \Phi_{\state} \right\rangle_F \right] \right\}.
	\label{eq:galerk_opt}
\end{align}

%
%Let us vectorize the coefficients $\{\hilbertcoeff\}_{i=0}^{r-1} \in \mc{S}$ and basis functions %in the separable Hilbert space $\{\hilbertparam\}_{i=0}^{r-1} \in \mc{F}$
% as follows 
%	%
%	\begin{align}
%		\hilbertcoeffspace_r^T &= \left[\hilbertcoeff_0, \hilbertcoeff_1, \cdots, \hilbertcoeff_{r-1}\right], \,\,
%		%
%		\hilbertparamspace_r = \left[\hilbertparam_0, \hilbertparam_1, \cdots, \hilbertparam_{r-1}\right]^T.
%	\end{align}
%

\begin{remark}
	Note that $\hilbertparamspace_{\state}$ are the spatial derivatives of $\{\hilbertparam\}_{i=0}^{r-1}$ w.r.t $\state$. Furthermore, we choose not to cancel out the coefficients $\hilbertcoeffspace_r^T$ in \eqref{eq:galerk_opt} because we want to retain the characteristics of the original value function $\mathds{V}(\state, t)$ on the respective bases, $\left\{\hilbertcoeff_i, \hilbertparam_i\right\}_{i=0}^{r}$. At once, we see that if the optimal decomposition components $\{\hilbertcoeff_i\}_{i=1}^r \triangleq \hilbertcoeffspace_r$ and $\{\hilbertparam_i\}_{i=0}^{r-1} \triangleq \hilbertparamspace_r$ are known, equation \eqref{eq:galerk_opt} admits solutions  on finite meshes, rendering solution of the separable viscosity problem \eqref{eq:lower_hji_pde} straightforward  with the usual high precision and accuracy that Lax-Friedrichs schemes afford~\cite{CrandallLaxFriedrichs, CrandallFractional, Crandall1984, OsherShuENO}. 
\end{remark}


\subsection{Galerkin HJI Approximation Under Separable Dynamics}
%
Now, suppose that the dynamics $f(t; \state, \control, \disturb)$ from \eqref{eq:sys_dyn} is separable into its state, control, and disturbance components in an additive manner as follows,
%
\begin{align}
	\dot{\state} &= f(t; \state, \control, \disturb) \\
				 &= f(t; {\state}) \state(t) + f(t; \control) \control(t) + f(t; \disturb) \disturb(t),
	\label{eq:separable_dyna}
\end{align}
%
where $f (t; \state), f(t; \control),$ and $f(t; \disturb)$ are the respective components of the system dynamics for the state, control law, and disturbance. This separable dynamics is typically observed for autonomous systems such as Dubins vehicles in relative coordinates, quadcopters, and many natural systems\footnote{Even when the separation in \eqref{eq:separable_dyna} is not possible globally, we can consider a perturbation $\delta \state$ about the state $\state$ along a nominal trajectory $\bar{\state}$ so that the system's locally linear state is iteratively measured with respect to $\state$ as it is commonly done in linear quadratic methods. We defer the treatment of these locally linearized dynamics to a future work. For an in-depth treatment, see \cite{DenhamDDP, Mitter1966, McReynolds1967, Jacobson1968new}.}. An example is a system of two Dubins cars in relative coordinates\cite{Merz1972} under constant linear speed and whose motion on is controlled by the relative orientation of the vehicles$v$ \ie 
%
\begin{align}
	\begin{bmatrix}
		\dot{x} \\ \dot{y} \\ \dot{\theta}
	\end{bmatrix} = \begin{bmatrix}
	x \cos \theta \\ y \sin \theta \\ \omega
\end{bmatrix}, \quad \omega \in \mathcal{U}
\end{align}
%
with state $\state=\left(x, y, \theta\right)$ whose components are the positions $(x, y)$ and heading $\theta$.
%
\begin{theorem}
	If the system dynamics are separable as in \eqref{eq:separable_dyna}, then the right hand side of the variational HJI problem that admits separable solutions is given by
	%
	\begin{equation}
		\begin{split}
-\hilbertcoeffspace_r^T \, \dot{\hilbertparamspace}_r =
			\min \left\{0,\left\langle f(t; {\state}) \state(t),   \hilbertcoeffspace_r^T \, \hilbertparamspace_{\state} \right\rangle 
		%
		\right. \\
		\left.
		%
		+ \max_{\control \in \mathcal{U}} \, \left\langle f(t; \control) \control(t), \hilbertcoeffspace_r^T \, \hilbertparamspace_{\state} \right\rangle 
		%
		\right. \\ 
		\left.  
		%
		+ \min_{\disturb \in \mathcal{V}}\, \left\langle   f(t; \disturb) \disturb(t),  \hilbertcoeffspace_r^T \, \hilbertparamspace_{\state} \right\rangle \right\}.
		\end{split}
	\end{equation}
\label{th:separable}
\end{theorem}

\begin{proof}
Putting \eqref{eq:separable_dyna} into \eqref{eq:galerk_opt}, we find that
%
\begin{align}
	\begin{split}
		-\hilbertcoeffspace_r^T \, \dot{\hilbertparamspace}_r  &= \min \left\{0, \max_{\control \in \mathcal{U}} \, \min_{\disturb \in \mathcal{V}} \left\langle \left[f(t; {\state}) \state(t) + f(t; \control) \control(t) \right. \right. \right. \\
		& \quad   \left. \left. \left. + f(t; \disturb) \disturb(t) \right],   \, \hilbertcoeffspace_r^T \, \hilbertparamspace_{\state}  \right\rangle \right\},
	\end{split}
\end{align}
%
so that the term on the right hand side becomes
%
\begin{align}
	\begin{split}
		\min \left\{0,\left\langle f(t; {\state}) \state(t),   \hilbertcoeffspace_r^T \, \hilbertparamspace_{\state} \right\rangle
		%
		+ \max_{\control \in \mathcal{U}} \, \left\langle f(t; \control) \control(t), \right. \right. \\ 
		\left. \left.  \hilbertcoeffspace_r^T \, \hilbertparamspace_{\state} \right\rangle 
		%
		+ \min_{\disturb \in \mathcal{V}}\, \left\langle   f(t; \disturb) \disturb(t),  \hilbertcoeffspace_r^T \, \hilbertparamspace_{\state} \right\rangle \right\}.
	\end{split}
	  \label{eq:galerk_separable}
\end{align}
%
\textit{A fortiori} we have the rhs of \eqref{eq:prob_statement} as \eqref{eq:galerk_separable} if the dynamics admits separability of the form \eqref{eq:separable_dyna}.
\end{proof}
%

The attractiveness of \eqref{th:separable} is that the respective Hamiltonians can be parallelized on multiple cores during iterations of the decomposition and then assembled on a centralized node to accelerate computation with for example, the alternating direction method of multipliers~\cite{BoydADMM}. \todo{This is treated and an example is given in \autoref{sec:admm}} 

Similar to \eqref{eq:galerk_opt}, \eqref{eq:galerk_separable} can be resolved on a mesh. However, when the dynamics are separable as in the foregoing, the saddle point necessary condition \ie \eqref{eq:saddle_points} allows us to find an analytic solution. \todo{Future work.}
%
\todo{Under development. A future paper?}

\subsection{Galerkin HJI Approximation Under $H$-$\infty$ Worst Disturbance Control}
\label{sec:hinfty}

\todo{Under Development}


\subsection{PGD Decomposition Scheme}%High-Order Quasi-Optimal Tensor Decompositions}
%
%The authors of~\cite{DecompChenHerbert} proposed a  local scheme that decomposed BRS and BRTs by the union and intersection of points on grid substructures, for basic geometric primitives, these projection and back-projections work reasonably well. For higher-order systems, where there is coupling among the subsystem modes and with BRS/BRTs whose geometry are nonlinear, low-rank approximations of tensor-structured BRS/BRTs can provide reasoned state relationships between decomposed structure and the global BRTs. %For high-dimensional systems, decomposition of reachable sets and tubes seem reasonable given recent work that explores this locally~\cite{DecompChenHerbert}.

%\begin{figure}[tb!]
%	\centering 
%	\includegraphics[width=\columnwidth]{figures/cylinder_2d.jpg}
%	\caption{Implicit representation of a value function defined as a signed distance function on a grid's interface.}
%	\label{fig:value_cyl}
%\end{figure}
%
%\begin{figure}[tb!]
%	\centering 
%	\includegraphics[width=\columnwidth]{figures/rect4_2d.jpg}
%	\caption{Implicit representation of a value function defined as a signed distance from grid points to an interface specified on the grid.}
%	\label{fig:value_rect}
%\end{figure}

%$\mathds{V}$,  implicitly defined as a signed distance to points on the grid on the left of \autoref{fig:value_rect}. The zero level set of this value function is  the right inset. A BRS/BRT defined by this hyperrectangle only span a finite rank of the state space (the mesh) on which $\mathds{V}$) is defined. 

It now remains for us to establish an optimal way to compute the basis and coefficients of the optimal Galerkin decomposition in \eqref{eq:galerk_opt} and \eqref{eq:galerk_separable}. %\todo{First, we introduce the following multilinear mappings:
%	%
%	\begin{itemize}
%		\item Let $S_r: \mc{F}  \rightarrow \mc{S}$ be the multilinear transformation from a real-valued state in $\phi \in \mc{F}$ to $\psi \in \mc{S}$ \ie,
%		\begin{align}
%			\amalg(\valuefunc_{r-1}+\phi \psi, \phi \psi_1^\star + \phi_1^\star \psi,\psi_2^\star \cdots \, \psi_r^\star) &= \uplus(\phi^\star \psi),\\
%			&\qquad \forall \, \phi^\star \in \mc{F}.
%		\end{align}
%		%
%		\item Let $F_r: \mc{S}  \rightarrow \mc{F}$ be the multilinear transformation from  $\hilbertcoeff \in \mc{S}$ to $\hilbertparam \in \mc{F}$ \ie,
%		\begin{align}
%			\amalg(\valuefunc_{r-1}+\phi \psi_0, \phi \psi_1, \cdots \phi \, \psi_r) = \uplus(\phi^\star \psi), \forall \, \phi^\star \in \mc{F}.
%		\end{align}
%	\end{itemize}
%}
%
\todo{we establish a Lemma due to~\cite{DeLathauwer2000} that allows every tensor $\mathds{V}$ to admit a higher-order singular value decomposition}. In our treatment, we resort to higher-order singular value decomposition (HOSVD)~\cite{Tucker66}\footnote{In his original work, Tucker only prescribed the decomposition of a tensor for up to 3 modes.}, extended to $N$-way tensors by~\cite{KapteynNWayTensors1986}. This consists in decomposing it into the product of a core tensor, $\valuecore \in \mathbb{R}^{R_0 \times R_1 \times \cdots \times R_n}, \, (R_n \le I_n)$, and \textit{unit norm factor matrices}, $\bm{U}_{n=0}^{N-1}$ of size $I_n \times R_n$ are of  $\mathds{V}$ (where $\valuetensor \in \mathbb{R}^{I_0 \times I_1 \times \cdots \times I_n}$)
%
\begin{subequations}
	\begin{align}
		\mathds{V}  &\approx \mathds{\hat{V}}  \\
		&= \valuecore \otimes_0 \mathbf{U}_0 \otimes_1 \mathbf{U}_1 \otimes_2 \mathbf{U}_2 \cdots \otimes_{N-2} \mathbf{U}_{N-1}.
	\end{align}
	\label{eq:tucker}
\end{subequations}
%
\noindent The matrices $\mathbf{U}_0 \in \mathbb{R}^{I_0 \times R_0}, \, \mathbf{U}_1 \in \mathbb{R}^{I_1 \times R_1}, \cdots \mathbf{U}_{N-1}$ can be seen as representing the influence of the principal components of each mode of $\mathds{V}$ on the \textit{core tensor} $\valuecore$.  Put differently, in dynamical systems parlance, this can be seen as the influence of a subsystem agent's dynamics (on a value function subspace)  on the overall value function of all interacting agents. The entries of $\valuecore$ denotes level of interaction between the different components $\bm{U}_n$. They can be thought of as the critical mass of the system's interaction -- encoding the objectives of the separate dynamical systems that share a large cyberphysical system space. The decomposition outlined in \eqref{eq:tucker} can be obtained via the following minimization problem
%
\begin{align}
	\min_{\valuecore,\, \mathbf{U}_0, \cdots, \mathbf{U}_{N-1}} &\| \valuetensor
	 - \valuecore \otimes_0 \mathbf{U}_0 \cdots \otimes_{N-2} \mathbf{U}_{N-1} \|^2_2 \nonumber\\
	 \text{subject to } &\valuecore \in \bb{R}^{R_0 \times R_1 \cdots \times R_{n-1}} \nonumber \\
	\text{ and orthonormal } &\{\mathbf{U}\}_{n=0}^{N-1} \in \bb{R}^{I_n\times R_n}. 
	 \label{min:tucker_decomp}
\end{align}
%
%The minimization problem in \ref{min:tucker_decomp} can be resolved efficiently with \autoref{alg:tucker_power_iter} -- an adaptation of 

\begin{algorithm}[tb!]
	\caption{Value Function Decomposition  
		\label{alg:tucker_power_iter}}
	\begin{algorithmic}[1]
		\Function{ValuePower}{$\valuetensor, \epsilon$}
		\Comment{Fix $\epsilon$, convergence threshold.}
		%
		\State Initialize $\{U\}_{n=0}^{n=N-1}$ \label{line:left_dom_eigvec}
		\Comment{$\valuetensor_n$ left dominant single vecs.} 
		%
		\State Set $\delta = +\infty$
		\Comment{$\delta$: Least-squares fit quality.}
		%
		\While{$\delta  > \epsilon$}
		\For{$n=0, \cdots, N-1$}
		\State $\mathbf{Q}_n \leftarrow \valuetensor \otimes_0 \mathbf{U}_0^T \otimes_1 \mathbf{U}_1^T \cdots \otimes_{N-1} \mathbf{U}_{N-1}^T$.
		\State $\mathbf{U}_n \leftarrow \frac{\mathbf{Q}_n}{\|\mathbf{Q}_n\|_2}$.
		\EndFor
		\State $\delta \leftarrow \|\mathbf{U}_n^T - \mathbf{U}_{n-1}^T\|_2$
		\Comment{Convergence check.}
		\EndWhile
		\State $\valuecore \leftarrow \valuetensor \otimes_0 \mathbf{U}_0^T \otimes_1 \mathbf{U}_1^T \cdots \otimes_{N-1} \mathbf{U}_{N-1}^T$. \label{tucker_decomp:line_value_core}
		\State \Return{$\valuecore, \{\mathbf{U}_n\}_{n=0}^{N-1}$.}
		\EndFunction
	\end{algorithmic}
\end{algorithm}

An efficient means for finding the factor matrices of minimization problem in \ref{min:tucker_decomp}  is a power iterations algorithm similar to DeLathauwer et. al's \cite{DeLathauwer2000}. A compact representation of DeLathauwer et. al's power iteration algorithm for a general purpose tensor decomposition is presented in Algorithm \autoref{alg:tucker_power_iter}. On line \autoref{line:left_dom_eigvec}, the left dominant singular vector of $\valuetensor$ can be found via Tucker's ``Method I" for computing the rank-$\left(R_0, R_1, \cdots, R_{N-1}\right)$ decomposition and we refer readers to~\cite{Tucker66}. Otherwise, they can be randomly initialized as well.

On line \autoref{tucker_decomp:line_value_core} of Algorithm \autoref{alg:tucker_power_iter}, using the orthonormal components of \eqref{eq:tucker}, we compute the \textit{optimal core} $\valuecore$ of $\mathds{V}$  (we refer readers to \cite{DeLathauwer2000} for a derivation) as 
%
\begin{align}
		\valuecore  &= \mathds{V} \otimes_0 \mathbf{U}_0^T \otimes_1 \mathbf{U}_1^T \otimes_2 \mathbf{U}_2^T \cdots \otimes_{N-2} \mathbf{U}_{N-1}^T.
		\label{eq:core_tensor}
\end{align}
%
Substituting \eqref{eq:core_tensor} into \eqref{eq:tucker}, we find that the approximation of $\mathds{V}$ is  a projection of the original value function tensor along its respective $n$-basis modes onto the reduced system, i.e.
%
\begin{align}
	\mathds{\hat{V}}  &= \valuetensor \otimes_0 \mathbf{U}_0 \mathbf{U}_0^T    \cdots \otimes_{N-2} \mathbf{U}_{N-1} \mathbf{U}_{N-1}^T. 
	\label{eq:value_reduced}
\end{align}
%
Therefore, the approximation error is $\|\valuetensor - \mathds{\hat{V}}\|^2 = \|\mathds{V}\|^2 - \|\valuecore\|^2$ (as shown in Appendix \ref{app:ortho_proj_error}). 

In light of our Galerkin approximations and the validity of any resulting BRS or BRT approximation, we will work with a truncated decomposition of $\valuefunc$ at each step of the algorithm. Define the BRT or BRS up to a mode $r$ as $\mathds{L}_r\left[\left[T, 0\right], \mathcal{L}_0\right])$. These truncations correspond to the  decomposed value function on various subspaces of the system. Because we keep the nonanticipative  strategies of $\pursuer$, the reachable set is still overapproximated in the larger sense, whereupon the pursuer makes decisions about $\disturb$ with full knowledge of $\control(\tau)$ for $\tau \in \left[t, t_f\right]$. Within this tolerance, we want to ensure  A truncated decomposition up to mode-$r$ would consist of a \textit{partial core} $\valuecore_r$ and its corresponding orthonormal matrices $\bm{U}_0, \cdots, \bm{U}_r$, $r \in \left[R \right]$ defined as 
%
\begin{align}
	\valuecore_r = \mathds{V} \otimes_0  \mathbf{U}_0^T \otimes_1 \mathbf{U}_1^T \otimes_2 \mathbf{U}_2^T \cdots \otimes_{r-1} \mathbf{U}_{r}^T,
	\label{eq:partial_core}
\end{align}
%
so that the truncated value function (up to mode $r$) is 
%
\begin{subequations}
	\begin{align}
		\valuetensor_r &= \valuecore_r \otimes_0 \mathbf{U}_0 \otimes_1 \mathbf{U}_1 \otimes_2 \mathbf{U}_2 \cdots \otimes_{r-1} \mathbf{U}_{r} \label{eq:value_trunc} \\
		%
		&\equiv  \mathds{V} \otimes_0  \mathbf{U}_0\mathbf{U}_0^T \otimes_1  \cdots \otimes_{r-1} \mathbf{U}_{r}\mathbf{U}_r^T, \, r \in \left[R\right]
		\label{eq:tucker_trunc} 
	\end{align}
\end{subequations}
%
where \eqref{eq:tucker_trunc} is a result of putting \eqref{eq:partial_core} into \eqref{eq:value_trunc}.  

Now, revisiting the functions $\{\hilbertcoeff\}_{i=0}^{r-1}$ and $\{\hilbertparam\}_{i=0}^{r-1}$ in the previous two sections, a convenient way to compute the coefficients and basis functions that satisfy the Galerkin orthogonality criterium \eqref{eq:galerk_orth_crit} is to set 
%
\begin{align}
	\hilbertcoeff_r = \valuetensor \otimes_0 \mathbf{U}_0^T \cdots \otimes_{r-1} \mathbf{U}_r^T
	\label{eq:hilbert_coeff_compute}
\end{align}
%
and 
%
\begin{align}
	\hilbertparam_r = \valuecore_{r} \otimes_0 \mathbf{U}_0 \cdots \otimes_{r-1} \mathbf{U}_r,
	\label{eq:hilbert_basis_compute}
\end{align}
%
where again, $\mathbf{U}_0, \cdots \mathbf{U}_{N-1}$ are factor matrices obtained from Algorithm \ref{alg:tucker_power_iter}.

Algorithm \ref{alg:pgd-power} describes how we  compose the separable value function that satisfies \eqref{eq:bilinear_transform}. Lines \ref{algpgd:line_coeff} and \ref{algpgd:line_basis} 
describe the optimal resolution of the decomposition parameters, $\{\hilbertcoeff\}_{i=0}^{r-1}$, and basis functions, $\{\hilbertparam\}_{i=0}^{r-1}$. In line \ref{algpgd:line_hji_update} of the algorithm, the integral is solved using Total Variation Diminishing (TVD) Runge-Kutta scheme as described in \cite[\S3.5]{LevelSetsBook} (originally implemented in \cite{MitchellLSToolbox2007}, which we re-implement in CuPy~\cite{CuPy} as we leverage parallel computation). In addition, having $r$ as an input variable into the algorithm allows us to take advantage of warm-starting schemes, typical in Reinforcement learning schemes~\cite{FisacICRA} so that the algorithm need not be run in one fell-swoop. Partial BRT's and BRS's can be distributively learned on separate CPU/GPU cores, and later assembled on a centralized nodes to aid faster computation. \todo{To be developed}.
This may for example be similar to the alternating direction method of multipliers.

\begin{algorithm}[tb!]
	\caption{Iterative Scheme for Computing BRS/BRTs  
		\label{alg:pgd-power}}
	\begin{algorithmic}[1]
		\Function{IterativeBRT}{$\valuetensor_{r-1}, r_{max}$}
		\Comment{$r_{max}$: max. iter.}
		%
		%\State Set $\valuetensor_{r-1}= \langle \psi_{0}, \phi_{0}\rangle$
		%
		\While{$r  < r_{max}$}
		\State Compute $\hilbertcoeff_r$ from \eqref{eq:hilbert_coeff_compute}. % $ \valuetensor_{r-1} \otimes_0 \mathbf{U}_0^T \cdots \otimes_{r-1} \mathbf{U}_r^T$
		\label{algpgd:line_coeff}
		\State Compute $\hilbertparam_r$ from \eqref{eq:hilbert_basis_compute}. %$ \valuecore_{r-1} \otimes_0 \mathbf{U}_0 \cdots \otimes_{r-1} \mathbf{U}_r$
		\label{algpgd:line_basis}
		\State Set $\valuetensor_r \leftarrow  \valuetensor_{r-1} + \langle\psi_r,\, \phi_r \rangle_{\mc{F}}$
		\Comment{Update $\valuetensor_r$.}
		%
		\State Set $\langle \hilbertcoeffspace_r^T, \, \dot{\hilbertparamspace}_r \rangle_F \leftarrow \valuetensor_r$  \cf \eqref{eq:galerk_opt} or  \eqref{eq:galerk_separable}
		\label{algpgd:line_hji_update}
		%
		\State $\mathds{L}_r \leftarrow \int_{\mathcal{S}} \langle \hilbertcoeffspace_r^T, \, \dot{\hilbertparamspace}_r \rangle_{\mc{F}} d\state$. 
		\Comment{Partial BRS(T), $\mathds{L}_r$}
		%
		\State $r = r+1$.
		\Comment{Advance the decomposition.}
		\EndWhile
		\State \Return{$\{\mathds{L}\}_{r=0}^{r_{max}-1}$}
		\EndFunction
	\end{algorithmic}
\end{algorithm}

Once the respective backward reachable tubes are collected, we must stitch them together such that their composition satisfies the boundary condition of the initial value problem of \eqref{eq:lower_hji_pde}. \todo{Lekan: Discuss the treatments of the singular and dispersal surfaces here.}

In algorithm \ref{alg:pgd-power}, the maximum number of iterations, $r_{max}$ can be chosen in an informed way as highlighted below. First, we restate the following theorem  that allows us to provide a bound on the projection error.
%
\begin{theorem}{[Vannieuwenhoven, Vandebril, and Meerbergen]}\cite[Th. 5.1]{VannieuwenhovenTruncate2012}. 
	Suppose $\mathds{V}$ is a tensor of size $I_0 \times I_1 \times \cdots \times I_{N-1}$, approximated by $\mathds{\hat{V}}$ as in \eqref{eq:value_reduced}, the approximation error of \eqref{eq:galerk_orth_crit} is 
	%
	\begin{align}
		\|\mathds{V} &- \mathds{\hat{V}} \|^2_F = \| \pi_0 \, \mathds{V}  \|_F^2  + \|\pi_1 \,  \mathds{\tilde{V}}_0\|_F^2  + \cdots +\|\pi_{N-1} \,   \mathds{\tilde{V}}_{N-2} \|_F^2.
	\end{align}
	\label{theorem:hosvd}
	where $\mathds{\tilde{V}}_{N}$ is the partial core along mode $N$.
\end{theorem}
%
Furthermore, the approximation error is bounded by 
%
\begin{align}
	\| \mathds{V} - \mathds{\hat{V}} \|_F^2 \le \sum_{n = 0}^{N-1} \| \pi_n \mathds{V} \|_F^2.
	\label{eq:approx_error}
\end{align}
%
Equation \eqref{eq:approx_error} allows us to choose an approximation error that informs us about the level of information we want preserved on the decomposed $\mathds{V}$. Therefore, to  find a decomposition $\mathds{\hat{V}}$ of $\mathds{V}$ whose relative decomposition error is no greater than a certain $\epsilon\, > \,0$, we first unfold the tensor along one of its modes, compute the Gram matrix and then carry out an eigen decomposition:
%
\begin{align}
	\mathbf{G} \equiv \mathds{V}_{(n)} \, \mathds{V}_{(n)}^T = \mathbf{V} \mathbf{\Lambda} \mathbf{V}^T,
	\label{eq:eig_decomp}
\end{align}
%
where $\mathbf{\Lambda} = \text{diag}\left(\{\lambda_1, \lambda_2, \cdots, \lambda_{I_n}\}\right)$, and $\lambda_1 \ge \lambda_2 \ge \cdots > \lambda_{I_n} \ge 0$, and $\mathbf{V}$ contains the corresponding eigenvectors. Therefore, we can choose the orthonormal matrix $\mathbf{U}_n$ and the best rank   $R_{n-1}$ tensor at mode $n-1$ as 
%
\begin{align}
	\mathbf{U}_n = \mathbf{V}\left[:,0:R_{n-1}\right]  
\end{align}
%
where  
%
\begin{align}
	R_n &= \min_{R \in \left[ I_n \right]} R \nonumber \\
	\text{ subject to } &\sum_{i=R+1}^{I_n} \lambda_i \le \epsilon^2 \|\mathds{V}\|^2/N.
	\label{eq:rank_choosing}
\end{align}

Equation prescribes an informed way to choose the best rank $R_r, \, r \in [0, n-1]$ that satisfies the error bound in  \eqref{eq:approx_error}. An efficient way of implementing \eqref{eq:rank_choosing} is to take the cummulative sum of the diagonal terms $\lambda_i$ in  \eqref{eq:eig_decomp} and return the index where this is at least equal to the right hand side of \eqref{eq:rank_choosing}
%
\begin{align}
	\| \mathds{V} \otimes_{n} \left(\mathbf{I} - \mathbf{U}_n \mathbf{U}_n^T\right) \|_F^2 \le \epsilon^2 \frac{ \| \mathds{V}\|_F^2}{N}.
\end{align}
%


Whilst appealing, algorithm \ref{alg:pgd-power} utilizes the full value function at each step (\cf \eqref{eq:hilbert_coeff_compute}); therefore it does not easily lend itself to large-scale problems. In what follows, we propose an incrementally-constructed value function whereupon we work with the \textit{partial cores} of $\valuefunc$ at each step of the PGD iteration. In essence, we construct the next factor matrix based on the previously computed value core. 


\subsection{Informed Incremental Value Function Decompositions}
\label{subsec:inc_hosvd}

Next, we will leverage the sequentially-truncated high-order SVD of \cite{VannieuwenhovenTruncate2012} to device an iteratively refined decomposition scheme onto which we will project the high-order value function. Working with partial cores, at step $n$, we generate the next factor matrix based on $\mathds{\hat{V}}_{n-1}$. In particular, if the conditions of Theorem \ref{theorem:hosvd} hold, then
%
\begin{align}
	\|\mathds{V} - \mathds{\hat{V}}\|_F^2 &=  \| \mathds{V} \otimes_0 \left( \mathbf{I} - \mathbf{U}_0 \mathbf{U}_0^T \right)\|_F^2 +  \| \mathds{\tilde{G}}_0 \otimes_1   \nonumber \\
	& \, \left( \mathbf{I} - \mathbf{U}_1 \mathbf{U}_1^T \right)\|_F^2 \cdots  + \| \mathds{\tilde{G}}_{N-2} \otimes_{N-1} \nonumber \\
	& \, \left( \mathbf{I} - \mathbf{U}_{N-1} \mathbf{U}_{N-1}^T \right)\|_F^2.
\end{align}
%
For convenience, we reproduce algorithm I of \cite{VannieuwenhovenTruncate2012} in Algorithm \ref{alg:st-hosvd}.

\begin{savenotes}
	\begin{algorithm}[tb!]
		\caption{Incrementally Truncated Value Function\cite{TuckerMPI}.}
		\label{alg:st-hosvd}
		\begin{algorithmic}[1]
			\Function{TruncatedValue}{$\mathds{V}, \, \epsilon$}
			\Comment{%$\mathds{V}$: Full value function; 
				$\epsilon$: Desired accuracy.}
			%
			\State $\mathds{P} \leftarrow \mathds{V}$ 
			\Comment{$\mathds{P}:$ c.f. \eqref{eq:ttm}.}
			%			
			\For{$n=0, 1, \cdots, N-1$}
			\State$\mathbf{G} \leftarrow \mathds{P}_{(n)} \mathds{P}_{(n)}^T$
			\Comment{$\mathbf{G}$: Gram matrix.}
			%
			\State$(\bm{\Lambda}, \mathbf{W}) \leftarrow \text{eig } (\mathbf{G})$
			\Comment{eig: Eigen Decomposition.}
			%
			\State $R_{n} \leftarrow \min { R \in \left[ I_{n} \right] | \sum_{i=R+1}^{I_n} \lambda_i \le \epsilon^2 \|\mathds{V}\|^2_F / N }$
			%
			\State $\mathbf{U}_n \leftarrow \mathbf{W}\left[:,0:R_{n-1}\right]$
			\Comment{Orthonormal $\mathbf{U}_n$}
			%
			\State $\mathds{P} \leftarrow \mathbf{G} \otimes_n \mathbf{U}_n^T$
			\Comment{Update partial core.}
			\EndFor
			\State $\valuecore \leftarrow \mathds{P}$ 
			\Comment{$\valuecore$: Update core tensor.}
			%
			\State $\mathbf{U} \leftarrow \{\mathbf{U}_0, \cdots, U_{N-1} \}$
			\State \Return $(\valuecore, \mathbf{U})$
			\Comment{$\valuecore$: Core tensor; $\{\mathbf{U}_i\}_{i=0}^{N-1}$.}
			\EndFunction
		\end{algorithmic}
	\end{algorithm}
\end{savenotes}


\begin{figure}[tb!]
	\centering
	\includegraphics[width=\columnwidth]{figures/proj_error.jpg}
	\caption{Optimal basis rank given a user-defined projection error.}
	\label{fig:proj_error}
\end{figure}


\begin{comment}
At issue in our optimal analytic framework is the construction of \textit{a priori}  separated representations without knowing the solution or an approximation to the reachability problem. Ladev\`{e}ze's LArge Time INcrement (LATIN) nonlinear iterative solver~\cite{LadevezeBook} improves the memory and computational times for solving multiple linear evolution problems in global time. By robustly addressing history-dependent mechanical properties of equations, a domain decomposition scheme first separates a mechanical structure into substructures and their boundaries that are spatially local and possibly with nonlinear equations; an iterative and parallel scheme then generates kinematic admissibility for its equilibrium computations that are linear and possibly global in space variables. %  of local and nonlinear dynamics from global and linear steady state phenomena whereupon a spatio-temporal separation 
This allows important savings in computational time and problem size~\cite{BlanzeModular, ChampaneyLarge}.


%Each substructure is considered as a structure on its own and communicates only with its neighbouring interfaces. The local nonlinearities are treated in a local and mixed manner through a constitutive law associated with an interface. Thus, interfaces are bidimensional entities with their own behaviour and their own associated unknowns. An iterative scheme leads to the resolution of independent problems on each substructure and independent nonlinear mixed problems on the interfaces. This domain decomposition method also introduces some modularity when considering local nonlinearities through suited interfaces which easily model the technological reality (prestresses, gaps, unilateral contact, friction, rubber joints, etc.). ~\cite{ChampaneyLarge}


%We know that the optimality condition of the dynamic programming value in differential control theory implies that the value is the viscosity solution of the corresponding Hamilton-Jacobi-Bellman PDE~\cite{Lions1982}, and 

%
\begin{align}
H(x, u(x), Du(x)) = 0 \quad \text{ for } x \in \openset, \, u=z \text{ on } \partial \openset
\label{eq:dyna}
\end{align}
%
where $\openset$ is an open set in $\mathbb{R}^n$, and $u$ and $z$ are on the boundary of the open set \ie $\partial \openset$. For problems where the dynamics explicitly depend on time, we will pose the initial value problem for \eqref{eq:dyna} as the \textit{Cauchy problem} HJ equations, \ie,
%
\begin{align}
u_t & + H(x(t), t, u(x(t)), Du(x(t))) = 0,   \, \openset \times \left[0, T\right], \nonumber \\
u&=z \text{ on } \partial \openset \times \left(0, T\right], \quad u(x, 0) = u_0(x), \quad \in  \openset
\end{align}
%
where  $z$ and $u_0$ are known functions on the boundary and the flow field (or Hamiltonian)  $H(\cdot)$ is the mapping:  $H:\openset\times \mathbb{R}^m\times \reline^n \rightarrow \reline$, and  $Du$ is the spatial gradient of $u$ with respect to $x$
%
\begin{align}
Du = \left(\frac{\partial u}{\partial x_1}, \cdots, \frac{\partial u}{\partial x_n} \right).
\end{align}


A key insight into the  robustly optimal feasible solutions of trajectories in high dimensional state spaces is by characterized by extending Theorem 1.3 of \cite{Crandall1984}. Basically, we let a cell $\bar{\mathcal{X}} \subset \mathcal{X}$ within the partition space of the state be divisible into a union of two subsets  piecewise $C^1$ viscosity solutions of $H=0$ by dividing the open set, $\openset$, by a $C^1$ surface $\Gamma: \openset = \openset_{+} \cup \openset_{-} \cup \Gamma$ into two open subsets, $\openset_{+}$ and $\openset_{-}$]in the following theorem,
%
\begin{theorem}
Let $u \in C(\openset)$ and $u \in u_{+} \in \openset_{+} \cup \Gamma$, $u = u_{-} \cup \Gamma$ where $u_+$ and $u_{-} $ are class $C^1 \in \openset_{+} \cup \Gamma$, and $\openset_{-} \cup \Gamma$. Then $u$ is a viscosity solution of \eqref{eq:dyna} if the following holds:
\begin{enumerate}[(i)]
\item $u_{+}$ and $u_{-}$ are classical solutions of $H=0$ in $\openset_{+}$ and $\openset_{-}$ respectively, and 
%
\item if $x_0 \in \Gamma$, $Tx_0 = \{\tau \in \bb{R}^m: n(x_0) \cdot \tau = 0\}$ is the tangent space to $\Gamma$ at $x_0$ and $P_T$ is the orthogonal projection of $\bb{R}^m$ onto %Tx_0$, then 
\end{enumerate}
\label{th:viscosity_divide}
\end{theorem}



%\subsection{PGD Description}

\noindent Without loss of generality, we here describe an incremental proper generalized decomposition scheme on a three-dimensional Value function (where the 2D case is a degenerate). Denote by $\Psi_m = \{\Psi_i\}_{i-1}^m \in \left(\Xi\right)^m$ the set of space functions and $\Phi_m = \{\Phi_i\}_{i-1}^m \in \left(\mathcal{T}\right)^m$ the set of time functions of $u_m = \Psi_m \cdot \Phi_m$ in \eqref{eq:decomp_general}.

We start with a substructure of the state space, $u_{m-1}$ which is assumed to be known. A new residual $(\phi \times \psi) \in \Xi \times \mathcal{T}$ is an optimal couple that satisfies the double Galerkin orthogonality criterium, 
%
\begin{align}
	B(V^-_{m-1} + \psi \phi, \psi \phi^\star + \psi^\star \phi) &= L(\psi \phi^\star +  \phi^\star \psi), \nonumber \\
	&\qquad \ \forall \, \phi^\star \in \mathcal{T}, \,\, \forall \, \psi^\star \in \Xi
\end{align}

We consider the following space to time and time to space functional mappings
\begin{definition}
    $S_m: \mathcal{T} \rightarrow \Xi$ is the mapping that associates a function $\phi \in \mathcal{T}$, into a space function $\psi = S_m(\phi) \in \Xi$, given as
    \begin{align}
        B(V^-_{m-1} + \psi \phi, \psi^\star \phi) = L(\psi^\star, \phi), \quad \psi^\star \in \Xi.
    \end{align}
\end{definition}
%
\begin{definition}
    $T_m: \Xi \rightarrow  \mathcal{T} $ is the mapping that associates a function $\psi \in \Xi$, into a time function $\phi = \mathcal{T}_m(\psi) \in \mathcal{T}$,  so that
    %
    \begin{align}
        B(V^-_{m-1} + \psi \phi, \phi^\star \psi) = L(\phi^\star, \psi), \quad \phi^\star \in \mathcal{T}.
    \end{align}
\end{definition}
%
\end{comment}

